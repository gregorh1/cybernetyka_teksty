#!/bin/bash

# ğŸš€ AUTOMATYCZNY FINE-TUNING BIELIK-11B + CYBERNETYKA
# Kompletny skrypt uruchamiajÄ…cy wszystko w odpowiedniej kolejnoÅ›ci

echo "ğŸš€ FINE-TUNING BIELIK-11B z KORPUSEM CYBERNETYKI"
echo "=================================================="

# SprawdÅº czy jesteÅ›my w odpowiednim folderze
if [ ! -f "cybernetyka_corpus.txt" ]; then
    echo "âŒ Brak pliku cybernetyka_corpus.txt"
    echo "   Uruchom skrypt w folderze z korpusem!"
    exit 1
fi

# SprawdÅº Python
echo "ğŸ Sprawdzam Python..."
python3 --version
if [ $? -ne 0 ]; then
    echo "âŒ Python3 nie znaleziony!"
    exit 1
fi

echo "âœ… Python OK"

# SprawdÅº NVIDIA
echo "ğŸ® Sprawdzam NVIDIA GPU..."
nvidia-smi > /dev/null 2>&1
if [ $? -ne 0 ]; then
    echo "âŒ NVIDIA GPU lub sterowniki nie znalezione!"
    echo "   Zainstaluj sterowniki NVIDIA i CUDA"
    exit 1
fi

echo "âœ… NVIDIA GPU wykryte"

# Krok 1: Setup Å›rodowiska
echo ""
echo "ğŸ“¦ KROK 1: Setup Å›rodowiska"
echo "----------------------------"
python3 setup_environment.py

if [ $? -ne 0 ]; then
    echo "âŒ Setup Å›rodowiska nieudany!"
    echo "   SprawdÅº bÅ‚Ä™dy i uruchom ponownie"
    exit 1
fi

echo "âœ… Åšrodowisko skonfigurowane"

# Krok 2: SprawdÅº korpus
echo ""
echo "ğŸ“š KROK 2: Sprawdzam korpus"
echo "---------------------------"
CORPUS_SIZE=$(wc -c < cybernetyka_corpus.txt)
CORPUS_MB=$(echo "scale=1; $CORPUS_SIZE / 1024 / 1024" | bc)

echo "ğŸ“„ Korpus: $CORPUS_MB MB"
if (( $(echo "$CORPUS_MB < 5" | bc -l) )); then
    echo "âŒ Korpus za maÅ‚y! Oczekiwano ~11MB"
    exit 1
fi

echo "âœ… Korpus gotowy do trenowania"

# Krok 2.5: WybÃ³r metody fine-tuningu
echo ""
echo "ğŸ”§ KROK 2.5: WybÃ³r metody fine-tuningu"
echo "-------------------------------------"
echo "DostÄ™pne sÄ… dwie metody:"
echo ""
echo "1ï¸âƒ£  UNSLOTH (Szybkie) - 2x szybsze, ale moÅ¼e mieÄ‡ problemy z kompatybilnoÅ›ciÄ…"
echo "   âœ… Szybsze trenowanie (~2-4h)"
echo "   âœ… Mniejsze zuÅ¼ycie VRAM"
echo "   âŒ Problemy z wersjami PyTorch/CUDA"
echo ""
echo "2ï¸âƒ£  STANDARD (Stabilne) - Wolniejsze, ale bardzo stabilne"
echo "   âœ… Wysoka kompatybilnoÅ›Ä‡"
echo "   âœ… Åatwiejsze debugowanie"
echo "   âŒ Wolniejsze trenowanie (~3-6h)"
echo ""

# Test Unsloth
echo "ğŸ§ª Sprawdzam dostÄ™pnoÅ›Ä‡ Unsloth..."
python3 -c "
try:
    from unsloth import FastLanguageModel
    print('âœ… Unsloth dziaÅ‚a!')
    exit(0)
except Exception as e:
    print('âŒ Unsloth problem:', str(e)[:100] + '...')
    exit(1)
" 2>/dev/null

UNSLOTH_AVAILABLE=$?

if [ $UNSLOTH_AVAILABLE -eq 0 ]; then
    echo ""
    echo "ğŸ¤– Wybierz metodÄ™ fine-tuningu:"
    echo "   [1] Unsloth (Zalecane - szybkie)"
    echo "   [2] Standard (Stabilne)"
    echo ""
    read -p "TwÃ³j wybÃ³r [1-2]: " CHOICE
    
    case $CHOICE in
        1)
            FINETUNE_METHOD="unsloth"
            FINETUNE_SCRIPT="finetune_bielik.py"
            echo "âœ… Wybrano: Unsloth (szybkie trenowanie)"
            ;;
        2)
            FINETUNE_METHOD="standard"
            FINETUNE_SCRIPT="finetune_bielik_standard.py"
            echo "âœ… Wybrano: Standard (stabilne trenowanie)"
            ;;
        *)
            echo "âŒ NieprawidÅ‚owy wybÃ³r, uÅ¼ywam Standard"
            FINETUNE_METHOD="standard"
            FINETUNE_SCRIPT="finetune_bielik_standard.py"
            ;;
    esac
else
    echo "âš ï¸  Unsloth niedostÄ™pny, uÅ¼ywam metody Standard"
    FINETUNE_METHOD="standard"
    FINETUNE_SCRIPT="finetune_bielik_standard.py"
fi

# Krok 3: Fine-tuning
echo ""
echo "ğŸ”¥ KROK 3: Fine-tuning (moÅ¼e zajÄ…Ä‡ 2-6 godzin)"
echo "----------------------------------------------"
echo "ğŸ“‹ Metoda: $FINETUNE_METHOD"
echo "ğŸ“„ Skrypt: $FINETUNE_SCRIPT"
echo "â° Rozpoczynam: $(date)"
echo ""

# Monitorowanie w tle
{
    sleep 60
    while pgrep -f "$FINETUNE_SCRIPT" > /dev/null; do
        echo "âš¡ $(date): Trenowanie w toku... (Monitor: nvidia-smi)"
        sleep 300  # Co 5 minut
    done
} &

# Uruchom fine-tuning
python3 $FINETUNE_SCRIPT

FINETUNE_EXIT=$?

# Zatrzymaj monitorowanie
kill $! 2>/dev/null

echo ""
echo "â° ZakoÅ„czono: $(date)"

if [ $FINETUNE_EXIT -ne 0 ]; then
    echo "âŒ Fine-tuning nieudany!"
    echo "   SprawdÅº logi w folderze outputs/"
    exit 1
fi

echo "âœ… Fine-tuning zakoÅ„czony pomyÅ›lnie!"

# Krok 4: Test modelu
echo ""
echo "ğŸ§ª KROK 4: Test wytrenowanego modelu"
echo "------------------------------------"

if [ -d "bielik-cybernetyka-lora" ]; then
    python3 test_finetuned_model.py
    
    if [ $? -eq 0 ]; then
        echo "âœ… Model dziaÅ‚a poprawnie!"
    else
        echo "âš ï¸  Problemy z testowaniem modelu"
    fi
else
    echo "âŒ Folder modelu nie znaleziony!"
fi

# Podsumowanie
echo ""
echo "ğŸ‰ FINE-TUNING ZAKOÅƒCZONY!"
echo "=========================="

if [ -d "bielik-cybernetyka-lora" ]; then
    MODEL_SIZE=$(du -sh bielik-cybernetyka-lora | cut -f1)
    echo "ğŸ“‚ Model LoRA: bielik-cybernetyka-lora/ ($MODEL_SIZE)"
fi

if [ -d "bielik-cybernetyka" ]; then
    GGUF_SIZE=$(du -sh bielik-cybernetyka | cut -f1)
    echo "ğŸ“¦ Model GGUF: bielik-cybernetyka/ ($GGUF_SIZE)"
fi

echo ""
echo "ğŸš€ NASTÄ˜PNE KROKI:"
echo "1. Testuj model: python3 test_finetuned_model.py"
echo "2. UÅ¼yj w kodzie: ./bielik-cybernetyka-lora/"
echo "3. Import do Ollama: ollama create bielik-cybernetyka -f bielik-cybernetyka/model.gguf"
echo ""
echo "ğŸ’¡ Model jest ekspertem w cybernetyce polskiej szkoÅ‚y (Kossecki + Mazur)!"
echo "ğŸ“Š Metoda uÅ¼yta: $FINETUNE_METHOD"

echo "ğŸ“Š LOGI:"
echo "- Training: outputs/training.log"
echo "- GPU usage: nvidia-smi" 