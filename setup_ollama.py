#!/usr/bin/env python3
"""
Setup fine-tuned Bielik model for Ollama integration
"""

import os
import subprocess
import shutil
from pathlib import Path
import argparse

def check_ollama_installed():
    """Check if Ollama is installed"""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        print(f"âœ… Ollama zainstalowane: {result.stdout.strip()}")
        return True
    except FileNotFoundError:
        print("âŒ Ollama nie jest zainstalowane")
        print("   Zainstaluj z: https://ollama.ai/download")
        return False

def merge_lora_adapter(model_path, output_path):
    """Merge LoRA adapter with base model"""
    print(f"ğŸ”„ ÅÄ…czÄ™ LoRA adapter z modelem bazowym...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        
        # Load base model
        base_model_name = "speakleash/Bielik-11B-v2.2-Instruct"
        print(f"   ÅadujÄ™ model bazowy: {base_model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype="auto",
            device_map="cpu"  # Use CPU for merging
        )
        
        # Load LoRA adapter
        print(f"   ÅadujÄ™ adapter LoRA z: {model_path}")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # Merge adapter
        print("   ÅÄ…czÄ™ adapter z modelem...")
        merged_model = model.merge_and_unload()
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Save merged model
        print(f"   ZapisujÄ™ poÅ‚Ä…czony model do: {output_path}")
        merged_model.save_pretrained(output_path, safe_serialization=True)
        tokenizer.save_pretrained(output_path)
        
        print("âœ… Model poÅ‚Ä…czony pomyÅ›lnie")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d Å‚Ä…czenia modelu: {e}")
        return False

def convert_to_gguf(model_path, output_path):
    """Convert model to GGUF format for Ollama"""
    print(f"ğŸ”„ KonwertujÄ™ model do formatu GGUF...")
    
    # Check if llama.cpp convert script exists
    convert_script = "convert-hf-to-gguf.py"
    
    if not shutil.which("python3"):
        print("âŒ Python3 nie znaleziony")
        return False
    
    # Try to find llama.cpp converter
    common_paths = [
        "./llama.cpp/convert_hf_to_gguf.py",
        "../llama.cpp/convert_hf_to_gguf.py",
        f"{os.path.expanduser('~')}/llama.cpp/convert_hf_to_gguf.py"
    ]
    
    converter_path = None
    for path in common_paths:
        if os.path.exists(path):
            converter_path = path
            break
    
    if not converter_path:
        print("âŒ Nie znaleziono konwertera llama.cpp")
        print("ğŸ’¡ Instrukcje instalacji:")
        print("   git clone https://github.com/ggerganov/llama.cpp")
        print("   cd llama.cpp")
        print("   pip install -r requirements.txt")
        return False
    
    try:
        cmd = [
            "python3", converter_path,
            model_path,
            "--outtype", "f16",
            "--outfile", output_path
        ]
        
        print(f"   WykonujÄ™: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Konwersja do GGUF zakoÅ„czona")
            return True
        else:
            print(f"âŒ BÅ‚Ä…d konwersji: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d konwersji: {e}")
        return False

def create_modelfile(model_name, gguf_path, output_path, params=None):
    """Create Ollama Modelfile with configurable parameters"""
    print(f"ğŸ“ TworzÄ™ Modelfile dla Ollama...")
    
    # Default parameters (conservative, anti-hallucination)
    if params is None:
        params = {
            'temperature': 0.3,      # Lower for better quality
            'top_p': 0.7,           # More focused
            'repeat_penalty': 1.3,   # Stronger anti-repetition
            'top_k': 50             # Controlled sampling
        }
    
    modelfile_content = f"""FROM {gguf_path}

TEMPLATE \"\"\"UÅ¼ytkownik: {{ .Prompt }}

Asystent cybernetyki: \"\"\"

PARAMETER temperature {params['temperature']}
PARAMETER top_p {params['top_p']}
PARAMETER repeat_penalty {params['repeat_penalty']}
PARAMETER top_k {params['top_k']}

SYSTEM \"\"\"JesteÅ› ekspertem w dziedzinie cybernetyki, szczegÃ³lnie w teorii opracowanej przez Mariana Mazura i JÃ³zefa Kosseckiego. Odpowiadasz szczegÃ³Å‚owo na pytania dotyczÄ…ce cybernetyki spoÅ‚ecznej, teorii systemÃ³w, sprzÄ™Å¼eÅ„ zwrotnych, homeostazy i innych zagadnieÅ„ cybernetycznych. Udzielasz wyczerpujÄ…cych, akademickich odpowiedzi z przykÅ‚adami i objaÅ›nieniami. Nie kopiuj dosÅ‚ownie tekstu z korpusu, lecz wyjaÅ›niaj koncepcje wÅ‚asnymi sÅ‚owami.\"\"\"
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"âœ… Modelfile zapisany: {output_path}")
    print(f"ğŸ›ï¸  Parametry: temp={params['temperature']}, top_p={params['top_p']}, repeat_penalty={params['repeat_penalty']}")
    return True

def import_to_ollama(modelfile_path, model_name):
    """Import model to Ollama"""
    print(f"ğŸ¦™ ImportujÄ™ model do Ollama jako '{model_name}'...")
    
    try:
        cmd = ["ollama", "create", model_name, "-f", modelfile_path]
        print(f"   WykonujÄ™: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"âœ… Model '{model_name}' zaimportowany do Ollama")
            print(f"ğŸ’¡ UÅ¼yj: ollama run {model_name}")
            return True
        else:
            print(f"âŒ BÅ‚Ä…d importu: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d importu: {e}")
        return False

def test_ollama_model(model_name):
    """Test the imported model"""
    print(f"ğŸ§ª TestujÄ™ model w Ollama...")
    
    try:
        test_prompt = "Co to jest sprzÄ™Å¼enie zwrotne w cybernetyce?"
        cmd = ["ollama", "run", model_name, test_prompt]
        
        print(f"   Test prompt: {test_prompt}")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            print(f"âœ… Test zakoÅ„czony sukcesem:")
            print(f"   OdpowiedÅº: {result.stdout[:200]}...")
            return True
        else:
            print(f"âŒ Test nieudany: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("â° Test przekroczyÅ‚ limit czasu")
        return False
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d testu: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Setup fine-tuned model for Ollama")
    parser.add_argument("--model-path", default="./bielik-cybernetyka-lora",
                       help="ÅšcieÅ¼ka do fine-tuned modelu")
    parser.add_argument("--model-name", default="bielik-cybernetyka",
                       help="Nazwa modelu w Ollama")
    parser.add_argument("--skip-merge", action="store_true",
                       help="PomiÅ„ Å‚Ä…czenie LoRA (jeÅ›li juÅ¼ wykonane)")
    parser.add_argument("--skip-convert", action="store_true",
                       help="PomiÅ„ konwersjÄ™ GGUF (jeÅ›li juÅ¼ wykonana)")
    parser.add_argument("--temperature", type=float, default=0.3,
                       help="Temperature dla Ollama (default: 0.3)")
    parser.add_argument("--top-p", type=float, default=0.7,
                       help="Top-p dla Ollama (default: 0.7)")
    parser.add_argument("--repeat-penalty", type=float, default=1.3,
                       help="Repeat penalty dla Ollama (default: 1.3)")
    parser.add_argument("--top-k", type=int, default=50,
                       help="Top-k dla Ollama (default: 50)")
    parser.add_argument("--preset", choices=["conservative", "balanced", "creative"],
                       default="conservative", help="Preset parametrÃ³w")
    
    args = parser.parse_args()
    
    print("ğŸ¦™ SETUP MODELU DLA OLLAMA")
    print("=" * 40)
    
    # Parameter presets
    presets = {
        "conservative": {
            'temperature': 0.3,
            'top_p': 0.7,
            'repeat_penalty': 1.3,
            'top_k': 50
        },
        "balanced": {
            'temperature': 0.5,
            'top_p': 0.8,
            'repeat_penalty': 1.2,
            'top_k': 40
        },
        "creative": {
            'temperature': 0.7,
            'top_p': 0.9,
            'repeat_penalty': 1.1,
            'top_k': 40
        }
    }
    
    # Start with preset parameters
    ollama_params = presets[args.preset].copy()
    print(f"ğŸ›ï¸  UÅ¼ywam presetu: {args.preset}")
    
    # Override with any custom parameters provided
    ollama_params.update({
        'temperature': args.temperature,
        'top_p': args.top_p,  # argparse converts --top-p to args.top_p
        'repeat_penalty': args.repeat_penalty,
        'top_k': args.top_k
    })
    
    print(f"ğŸ“Š Parametry Ollama: {ollama_params}")
    
    # Check prerequisites
    if not check_ollama_installed():
        return
    
    # Define paths
    merged_path = f"{args.model_path}-merged"
    gguf_path = f"{args.model_name}.gguf"
    modelfile_path = f"{args.model_name}.Modelfile"
    
    # Step 1: Merge LoRA adapter
    if not args.skip_merge:
        if not merge_lora_adapter(args.model_path, merged_path):
            print("âŒ Nie udaÅ‚o siÄ™ poÅ‚Ä…czyÄ‡ modelu")
            return
    else:
        print("â­ï¸  Pomijam Å‚Ä…czenie LoRA")
    
    # Step 2: Convert to GGUF
    if not args.skip_convert:
        if not convert_to_gguf(merged_path, gguf_path):
            print("âŒ Nie udaÅ‚o siÄ™ skonwertowaÄ‡ do GGUF")
            print("ğŸ’¡ Alternatywa: uÅ¼yj online converters lub llama.cpp")
            return
    else:
        print("â­ï¸  Pomijam konwersjÄ™ GGUF")
    
    # Step 3: Create Modelfile
    if not create_modelfile(args.model_name, gguf_path, modelfile_path, ollama_params):
        return
    
    # Step 4: Import to Ollama
    if not import_to_ollama(modelfile_path, args.model_name):
        return
    
    # Step 5: Test model
    test_ollama_model(args.model_name)
    
    print(f"\nğŸ‰ SETUP ZAKOÅƒCZONY!")
    print(f"   Model dostÄ™pny jako: {args.model_name}")
    print(f"   UÅ¼yj: ollama run {args.model_name}")
    print(f"   W Open WebUI: dodaj model '{args.model_name}'")

if __name__ == "__main__":
    main() 