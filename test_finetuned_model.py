#!/usr/bin/env python3
"""
Test wytrenowanego modelu Bielik-11B + Cybernetyka
"""

def test_model():
    """Testuj wytrenowany model"""
    
    # Sprawd≈∫ ≈õrodowisko
    try:
        import torch
        from unsloth import FastLanguageModel
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA nie jest dostƒôpne!")
            return
            
    except ImportError as e:
        print(f"‚ùå Brak wymaganej biblioteki: {e}")
        print("   Uruchom: python3 setup_environment.py")
        return
    
    print("üß™ Testowanie modelu Bielik-Cybernetyka")
    print("=" * 50)
    
    # Za≈Çaduj model
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="./bielik-cybernetyka-lora",
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    
    # Przygotuj do inferencji
    FastLanguageModel.for_inference(model)
    
    # Pytania testowe
    test_questions = [
        "Co to jest cybernetyka spo≈Çeczna?",
        "Jak Mazur definiuje system autonomiczny?",
        "Jakie sƒÖ podstawowe zasady cybernetyki?",
        "Czym r√≥≈ºni siƒô cybernetyka od informatyki?",
        "Co to jest sprzƒô≈ºenie zwrotne w cybernetyce?",
        "Jak dzia≈ÇajƒÖ systemy sterowania spo≈Çecznego?",
        "Co to jest teoria uk≈Çad√≥w samodzielnych?",
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüî¨ Test {i}: {question}")
        print("-" * 40)
        
        # Format dla Bielik
        prompt = f"""<|start_header_id|>system<|end_header_id|>

Jeste≈õ ekspertem z zakresu cybernetyki spo≈Çecznej i og√≥lnej. Odpowiadasz na podstawie polskiej szko≈Çy cybernetycznej, w szczeg√≥lno≈õci prac J√≥zefa Kosseckiego i Mariana Mazura.<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # Tokenizuj
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
        
        # Generuj odpowied≈∫
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Dekoduj odpowied≈∫
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # WyciƒÖgnij tylko odpowied≈∫ asystenta
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            answer = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        else:
            answer = response[len(prompt):].strip()
        
        print(f"ü§ñ Odpowied≈∫: {answer}")
        print()

if __name__ == "__main__":
    test_model() 