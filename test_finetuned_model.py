#!/usr/bin/env python3
"""
Test wytrenowanego modelu Bielik-11B + Cybernetyka
"""

def test_model():
    """Testuj wytrenowany model"""
    
    # Sprawd≈∫ ≈õrodowisko
    try:
        import torch
        from peft import PeftModel, PeftConfig
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA nie jest dostƒôpne!")
            return
            
    except ImportError as e:
        print(f"‚ùå Brak wymaganej biblioteki: {e}")
        print("   Uruchom: python3 setup_environment.py")
        return
    
    print("üß™ Testowanie modelu Bielik-Cybernetyka")
    print("=" * 50)
    
    # METODA PEFT: Za≈Çaduj adapter config, potem bazowy model, potem zastosuj adapter
    print("üì• ≈Åadowanie konfiguracji LoRA adaptera...")
    adapter_config = PeftConfig.from_pretrained("./bielik-cybernetyka-lora")
    
    print("üì• ≈Åadowanie bazowego modelu Bielik-11B...")
    
    # Konfiguracja 4-bit quantization
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        adapter_config.base_model_name_or_path,  # speakleash/Bielik-11B-v2.2-Instruct
        torch_dtype=torch.bfloat16,
        device_map="auto",
        quantization_config=quantization_config,
    )
    
    print("üì• ≈Åadowanie tokenizera...")
    tokenizer = AutoTokenizer.from_pretrained("./bielik-cybernetyka-lora")
    
    # Upewnij siƒô, ≈ºe tokenizer ma pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("üîå Stosowanie LoRA adaptera...")
    model = PeftModel.from_pretrained(base_model, "./bielik-cybernetyka-lora")
    
    print("‚úÖ Model gotowy do test√≥w!")
    
    # Pytania testowe
    test_questions = [
        "Co to jest cybernetyka spo≈Çeczna?",
        "Jak Mazur definiuje system autonomiczny?", 
        "Jakie sƒÖ podstawowe zasady cybernetyki?",
        "Czym r√≥≈ºni siƒô cybernetyka od informatyki?",
        "Co to jest sprzƒô≈ºenie zwrotne w cybernetyce?",
        "Jak dzia≈ÇajƒÖ systemy sterowania spo≈Çecznego?",
        "Co to jest teoria uk≈Çad√≥w samodzielnych?",
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüî¨ Test {i}: {question}")
        print("-" * 40)
        
        # Format dla Bielik
        prompt = f"""<|start_header_id|>system<|end_header_id|>

Jeste≈õ ekspertem z zakresu cybernetyki spo≈Çecznej i og√≥lnej. Odpowiadasz na podstawie polskiej szko≈Çy cybernetycznej, w szczeg√≥lno≈õci prac J√≥zefa Kosseckiego i Mariana Mazura.<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # Tokenizuj
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
        
        # Generuj odpowied≈∫ z bardziej konserwatywymi parametrami
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.3,  # Ni≈ºsza temperatura = bardziej deterministyczne
                top_p=0.8,
                repetition_penalty=1.2,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # Dekoduj odpowied≈∫
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # WyciƒÖgnij tylko odpowied≈∫ asystenta
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            answer = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        else:
            answer = response[len(prompt):].strip()
        
        print(f"ü§ñ Odpowied≈∫: {answer}")
        print()

if __name__ == "__main__":
    test_model() 