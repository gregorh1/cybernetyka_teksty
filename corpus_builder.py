#!/usr/bin/env python3
"""
Tworzenie korpusu cybernetyki dla OpenWebUI
Łączy wszystkie pliki .txt w strukturalny korpus z metadanymi
"""

import os
import re
from pathlib import Path
import json
from datetime import datetime

def extract_metadata_from_txt(txt_path):
    """Wyciąga metadane z pliku tekstowego"""
    txt_path = Path(txt_path)
    
    metadata = {
        'filename': txt_path.name,
        'author_folder': txt_path.parent.name,
        'author': 'Józef Kossecki' if 'Kossecki' in str(txt_path) else 'Marian Mazur',
        'title': '',
        'original_file': '',
        'conversion_method': '',
        'pages': 0,
        'size_kb': round(txt_path.stat().st_size / 1024, 1),
        'words_estimate': 0,
        'topics': []
    }
    
    # Czytaj nagłówek pliku żeby wyciągnąć metadane
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            header = f.read(500)  # Pierwsze 500 znaków
            
        # Wyciągnij tytuł z pierwszej linii
        lines = header.split('\n')
        if lines and lines[0].startswith('=== '):
            metadata['title'] = lines[0].replace('=== ', '').replace(' ===', '').strip()
        
        # Szukaj informacji o oryginalnym pliku
        for line in lines[:10]:
            if 'Original PDF:' in line:
                metadata['original_file'] = line.split('Original PDF:')[1].strip()
            elif 'Converted from:' in line:
                metadata['original_file'] = line.split('Converted from:')[1].strip()
            elif 'Generated by:' in line:
                metadata['conversion_method'] = line.split('Generated by:')[1].strip()
            elif 'Total pages:' in line:
                try:
                    metadata['pages'] = int(re.search(r'\d+', line).group())
                except:
                    pass
                    
    except Exception as e:
        print(f"❌ Błąd czytania metadanych z {txt_path}: {e}")
    
    # Zgadnij tematy na podstawie nazwy pliku
    filename_lower = txt_path.stem.lower()
    topic_keywords = {
        'cybernetyka': ['cybernetyka', 'cybernetic'],
        'sterowanie': ['sterowanie', 'sterowniczych', 'sterowania'],
        'informacja': ['informacji', 'informacyjn'],
        'społeczność': ['spoleczn', 'społeczn'],
        'system': ['system', 'układów', 'ukladow'],
        'nauka': ['nauki', 'naukow'],
        'teoria': ['teoria', 'teorii'],
        'filozofia': ['filozofi', 'poznawcz'],
        'polityka': ['polityk', 'opozycj'],
        'cywilizacja': ['cywilizacj'],
        'charakterologia': ['charakter'],
        'edukacja': ['szkola', 'nauczania', 'dydaktyk'],
        'historia': ['histori', 'dzieje']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in filename_lower for keyword in keywords):
            metadata['topics'].append(topic)
    
    return metadata

def estimate_words(text):
    """Przybliżona liczba słów w tekście"""
    # Usuń znaczniki stron i nagłówki
    text = re.sub(r'--- Page \d+ ---', '', text)
    text = re.sub(r'=== .* ===', '', text)
    
    # Policz słowa
    words = len(text.split())
    return words

def clean_text_content(text):
    """Czyści tekst z artefaktów OCR"""
    # Usuń nadmiernie powtarzające się znaki
    text = re.sub(r'(.)\1{5,}', r'\1\1\1', text)
    
    # Popraw typowe błędy OCR w polskim tekście
    ocr_fixes = {
        'ę': ['e', 'q'],
        'ą': ['a', 'q'],  
        'ś': ['s'],
        'ć': ['c'],
        'ź': ['z'],
        'ż': ['z'],
        'ń': ['n'],
        'ł': ['l'],
        'ó': ['o']
    }
    
    # Usuń dziwne znaki
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\'\/\n\r\t]', ' ', text)
    
    # Popraw wielokrotne spacje
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n +', '\n', text)
    
    return text.strip()

def create_unified_corpus():
    """Tworzy jednolity korpus ze wszystkich plików .txt"""
    
    print("📚 Tworzenie korpusu cybernetyki...")
    print("=" * 50)
    
    # Znajdź wszystkie pliki .txt
    txt_files = []
    for folder in ['TEXTS/autonom/Kossecki', 'TEXTS/autonom/Mazur']:
        if os.path.exists(folder):
            txt_files.extend(list(Path(folder).glob('*.txt')))
    
    if not txt_files:
        print("❌ Nie znaleziono plików .txt")
        return
    
    print(f"📁 Znaleziono {len(txt_files)} plików .txt")
    
    # Przygotuj strukturę korpusu
    corpus = {
        'metadata': {
            'title': 'Korpus Cybernetyki Polskiej',
            'description': 'Zbiór tekstów z zakresu cybernetyki społecznej i ogólnej autorstwa Józefa Kosseckiego i Mariana Mazura',
            'authors': ['Józef Kossecki', 'Marian Mazur'],
            'created': datetime.now().isoformat(),
            'total_files': len(txt_files),
            'total_size_kb': 0,
            'total_words': 0,
            'total_pages': 0,
            'topics': set()
        },
        'documents': []
    }
    
    unified_text = []
    unified_text.append("KORPUS CYBERNETYKI POLSKIEJ")
    unified_text.append("="*50)
    unified_text.append(f"Zbiór tekstów z zakresu cybernetyki społecznej i ogólnej")
    unified_text.append(f"Autorzy: Józef Kossecki, Marian Mazur")
    unified_text.append(f"Utworzony: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    unified_text.append(f"Liczba dokumentów: {len(txt_files)}")
    unified_text.append("")
    
    # Przetwórz każdy plik
    for txt_file in sorted(txt_files):
        print(f"🔄 Przetwarzam: {txt_file.name}")
        
        try:
            # Wyciągnij metadane
            metadata = extract_metadata_from_txt(txt_file)
            
            # Czytaj treść
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Wyczyść tekst
            cleaned_content = clean_text_content(content)
            
            # Oszacuj liczbę słów
            word_count = estimate_words(cleaned_content)
            metadata['words_estimate'] = word_count
            
            # Dodaj do korpusu
            corpus['documents'].append({
                'metadata': metadata,
                'content': cleaned_content[:2000] + '...' if len(cleaned_content) > 2000 else cleaned_content  # Fragment dla JSON
            })
            
            # Dodaj do zunifikowanego tekstu
            unified_text.append(f"\n{'='*80}")
            unified_text.append(f"DOKUMENT: {metadata['title'] or txt_file.stem}")
            unified_text.append(f"AUTOR: {metadata['author']}")
            unified_text.append(f"PLIK: {metadata['filename']}")
            unified_text.append(f"ROZMIAR: {metadata['size_kb']} KB")
            unified_text.append(f"SŁOWA: ~{word_count}")
            if metadata['topics']:
                unified_text.append(f"TEMATY: {', '.join(metadata['topics'])}")
            unified_text.append(f"{'='*80}")
            unified_text.append("")
            unified_text.append(cleaned_content)
            unified_text.append("")
            
            # Aktualizuj statystyki korpusu
            corpus['metadata']['total_size_kb'] += metadata['size_kb']
            corpus['metadata']['total_words'] += word_count
            corpus['metadata']['total_pages'] += metadata['pages']
            corpus['metadata']['topics'].update(metadata['topics'])
            
            print(f"✅ Dodano: {metadata['title'] or txt_file.stem} ({word_count} słów)")
            
        except Exception as e:
            print(f"❌ Błąd przetwarzania {txt_file}: {e}")
    
    # Konwertuj set na listę dla JSON
    corpus['metadata']['topics'] = list(corpus['metadata']['topics'])
    
    # Zapisz metadane korpusu
    with open('cybernetyka_corpus_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(corpus, f, indent=2, ensure_ascii=False)
    
    # Zapisz zunifikowany korpus tekstowy
    unified_content = '\n'.join(unified_text)
    with open('cybernetyka_corpus.txt', 'w', encoding='utf-8') as f:
        f.write(unified_content)
    
    # Zapisz także kompaktową wersję dla OpenWebUI
    compact_content = []
    compact_content.append("CYBERNETYKA POLSKA - KORPUS TEKSTÓW")
    compact_content.append("Józef Kossecki & Marian Mazur")
    compact_content.append("")
    
    for doc in corpus['documents']:
        meta = doc['metadata']
        compact_content.append(f"\n## {meta['title'] or meta['filename']}")
        compact_content.append(f"**Autor:** {meta['author']}")
        if meta['topics']:
            compact_content.append(f"**Tematy:** {', '.join(meta['topics'])}")
        compact_content.append("")
        compact_content.append(doc['content'][:1500] + "..." if len(doc['content']) > 1500 else doc['content'])
    
    with open('cybernetyka_corpus_compact.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(compact_content))
    
    # Podsumowanie
    print(f"\n📊 PODSUMOWANIE KORPUSU:")
    print(f"📚 Dokumenty: {corpus['metadata']['total_files']}")
    print(f"📄 Strony: ~{corpus['metadata']['total_pages']}")
    print(f"📝 Słowa: ~{corpus['metadata']['total_words']:,}")
    print(f"💾 Rozmiar: {corpus['metadata']['total_size_kb']:.1f} KB")
    print(f"🏷️  Tematy: {', '.join(corpus['metadata']['topics'])}")
    
    print(f"\n📄 Utworzone pliki:")
    print(f"📋 cybernetyka_corpus_metadata.json - metadane korpusu")
    print(f"📄 cybernetyka_corpus.txt - pełny korpus ({len(unified_content)/1024:.1f} KB)")
    compact_size = len('\n'.join(compact_content))/1024
    print(f"📄 cybernetyka_corpus_compact.txt - kompaktowa wersja ({compact_size:.1f} KB)")
    
    return corpus

if __name__ == "__main__":
    create_unified_corpus() 