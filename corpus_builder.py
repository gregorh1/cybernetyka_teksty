#!/usr/bin/env python3
"""
Tworzenie korpusu cybernetyki dla OpenWebUI
ÅÄ…czy wszystkie pliki .txt w strukturalny korpus z metadanymi
"""

import os
import re
from pathlib import Path
import json
from datetime import datetime

def extract_metadata_from_txt(txt_path):
    """WyciÄ…ga metadane z pliku tekstowego"""
    txt_path = Path(txt_path)
    
    metadata = {
        'filename': txt_path.name,
        'author_folder': txt_path.parent.name,
        'author': 'JÃ³zef Kossecki' if 'Kossecki' in str(txt_path) else 'Marian Mazur',
        'title': '',
        'original_file': '',
        'conversion_method': '',
        'pages': 0,
        'size_kb': round(txt_path.stat().st_size / 1024, 1),
        'words_estimate': 0,
        'topics': []
    }
    
    # Czytaj nagÅ‚Ã³wek pliku Å¼eby wyciÄ…gnÄ…Ä‡ metadane
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            header = f.read(500)  # Pierwsze 500 znakÃ³w
            
        # WyciÄ…gnij tytuÅ‚ z pierwszej linii
        lines = header.split('\n')
        if lines and lines[0].startswith('=== '):
            metadata['title'] = lines[0].replace('=== ', '').replace(' ===', '').strip()
        
        # Szukaj informacji o oryginalnym pliku
        for line in lines[:10]:
            if 'Original PDF:' in line:
                metadata['original_file'] = line.split('Original PDF:')[1].strip()
            elif 'Converted from:' in line:
                metadata['original_file'] = line.split('Converted from:')[1].strip()
            elif 'Generated by:' in line:
                metadata['conversion_method'] = line.split('Generated by:')[1].strip()
            elif 'Total pages:' in line:
                try:
                    metadata['pages'] = int(re.search(r'\d+', line).group())
                except:
                    pass
                    
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d czytania metadanych z {txt_path}: {e}")
    
    # Zgadnij tematy na podstawie nazwy pliku
    filename_lower = txt_path.stem.lower()
    topic_keywords = {
        'cybernetyka': ['cybernetyka', 'cybernetic'],
        'sterowanie': ['sterowanie', 'sterowniczych', 'sterowania'],
        'informacja': ['informacji', 'informacyjn'],
        'spoÅ‚ecznoÅ›Ä‡': ['spoleczn', 'spoÅ‚eczn'],
        'system': ['system', 'ukÅ‚adÃ³w', 'ukladow'],
        'nauka': ['nauki', 'naukow'],
        'teoria': ['teoria', 'teorii'],
        'filozofia': ['filozofi', 'poznawcz'],
        'polityka': ['polityk', 'opozycj'],
        'cywilizacja': ['cywilizacj'],
        'charakterologia': ['charakter'],
        'edukacja': ['szkola', 'nauczania', 'dydaktyk'],
        'historia': ['histori', 'dzieje']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in filename_lower for keyword in keywords):
            metadata['topics'].append(topic)
    
    return metadata

def estimate_words(text):
    """PrzybliÅ¼ona liczba sÅ‚Ã³w w tekÅ›cie"""
    # UsuÅ„ znaczniki stron i nagÅ‚Ã³wki
    text = re.sub(r'--- Page \d+ ---', '', text)
    text = re.sub(r'=== .* ===', '', text)
    
    # Policz sÅ‚owa
    words = len(text.split())
    return words

def clean_text_content(text):
    """CzyÅ›ci tekst z artefaktÃ³w OCR"""
    # UsuÅ„ nadmiernie powtarzajÄ…ce siÄ™ znaki
    text = re.sub(r'(.)\1{5,}', r'\1\1\1', text)
    
    # Popraw typowe bÅ‚Ä™dy OCR w polskim tekÅ›cie
    ocr_fixes = {
        'Ä™': ['e', 'q'],
        'Ä…': ['a', 'q'],  
        'Å›': ['s'],
        'Ä‡': ['c'],
        'Åº': ['z'],
        'Å¼': ['z'],
        'Å„': ['n'],
        'Å‚': ['l'],
        'Ã³': ['o']
    }
    
    # UsuÅ„ dziwne znaki
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\'\/\n\r\t]', ' ', text)
    
    # Popraw wielokrotne spacje
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n +', '\n', text)
    
    return text.strip()

def create_unified_corpus():
    """Tworzy jednolity korpus ze wszystkich plikÃ³w .txt"""
    
    print("ğŸ“š Tworzenie korpusu cybernetyki...")
    print("=" * 50)
    
    # ZnajdÅº wszystkie pliki .txt
    txt_files = []
    for folder in ['TEXTS/autonom/Kossecki', 'TEXTS/autonom/Mazur']:
        if os.path.exists(folder):
            txt_files.extend(list(Path(folder).glob('*.txt')))
    
    if not txt_files:
        print("âŒ Nie znaleziono plikÃ³w .txt")
        return
    
    print(f"ğŸ“ Znaleziono {len(txt_files)} plikÃ³w .txt")
    
    # Przygotuj strukturÄ™ korpusu
    corpus = {
        'metadata': {
            'title': 'Korpus Cybernetyki Polskiej',
            'description': 'ZbiÃ³r tekstÃ³w z zakresu cybernetyki spoÅ‚ecznej i ogÃ³lnej autorstwa JÃ³zefa Kosseckiego i Mariana Mazura',
            'authors': ['JÃ³zef Kossecki', 'Marian Mazur'],
            'created': datetime.now().isoformat(),
            'total_files': len(txt_files),
            'total_size_kb': 0,
            'total_words': 0,
            'total_pages': 0,
            'topics': set()
        },
        'documents': []
    }
    
    unified_text = []
    unified_text.append("KORPUS CYBERNETYKI POLSKIEJ")
    unified_text.append("="*50)
    unified_text.append(f"ZbiÃ³r tekstÃ³w z zakresu cybernetyki spoÅ‚ecznej i ogÃ³lnej")
    unified_text.append(f"Autorzy: JÃ³zef Kossecki, Marian Mazur")
    unified_text.append(f"Utworzony: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    unified_text.append(f"Liczba dokumentÃ³w: {len(txt_files)}")
    unified_text.append("")
    
    # PrzetwÃ³rz kaÅ¼dy plik
    for txt_file in sorted(txt_files):
        print(f"ğŸ”„ Przetwarzam: {txt_file.name}")
        
        try:
            # WyciÄ…gnij metadane
            metadata = extract_metadata_from_txt(txt_file)
            
            # Czytaj treÅ›Ä‡
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # WyczyÅ›Ä‡ tekst
            cleaned_content = clean_text_content(content)
            
            # Oszacuj liczbÄ™ sÅ‚Ã³w
            word_count = estimate_words(cleaned_content)
            metadata['words_estimate'] = word_count
            
            # Dodaj do korpusu
            corpus['documents'].append({
                'metadata': metadata,
                'content': cleaned_content[:2000] + '...' if len(cleaned_content) > 2000 else cleaned_content  # Fragment dla JSON
            })
            
            # Dodaj do zunifikowanego tekstu
            unified_text.append(f"\n{'='*80}")
            unified_text.append(f"DOKUMENT: {metadata['title'] or txt_file.stem}")
            unified_text.append(f"AUTOR: {metadata['author']}")
            unified_text.append(f"PLIK: {metadata['filename']}")
            unified_text.append(f"ROZMIAR: {metadata['size_kb']} KB")
            unified_text.append(f"SÅOWA: ~{word_count}")
            if metadata['topics']:
                unified_text.append(f"TEMATY: {', '.join(metadata['topics'])}")
            unified_text.append(f"{'='*80}")
            unified_text.append("")
            unified_text.append(cleaned_content)
            unified_text.append("")
            
            # Aktualizuj statystyki korpusu
            corpus['metadata']['total_size_kb'] += metadata['size_kb']
            corpus['metadata']['total_words'] += word_count
            corpus['metadata']['total_pages'] += metadata['pages']
            corpus['metadata']['topics'].update(metadata['topics'])
            
            print(f"âœ… Dodano: {metadata['title'] or txt_file.stem} ({word_count} sÅ‚Ã³w)")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przetwarzania {txt_file}: {e}")
    
    # Konwertuj set na listÄ™ dla JSON
    corpus['metadata']['topics'] = list(corpus['metadata']['topics'])
    
    # Zapisz metadane korpusu
    with open('cybernetyka_corpus_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(corpus, f, indent=2, ensure_ascii=False)
    
    # Zapisz zunifikowany korpus tekstowy
    unified_content = '\n'.join(unified_text)
    with open('cybernetyka_corpus.txt', 'w', encoding='utf-8') as f:
        f.write(unified_content)
    
    # Zapisz takÅ¼e kompaktowÄ… wersjÄ™ dla OpenWebUI
    compact_content = []
    compact_content.append("CYBERNETYKA POLSKA - KORPUS TEKSTÃ“W")
    compact_content.append("JÃ³zef Kossecki & Marian Mazur")
    compact_content.append("")
    
    for doc in corpus['documents']:
        meta = doc['metadata']
        compact_content.append(f"\n## {meta['title'] or meta['filename']}")
        compact_content.append(f"**Autor:** {meta['author']}")
        if meta['topics']:
            compact_content.append(f"**Tematy:** {', '.join(meta['topics'])}")
        compact_content.append("")
        compact_content.append(doc['content'][:1500] + "..." if len(doc['content']) > 1500 else doc['content'])
    
    with open('cybernetyka_corpus_compact.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(compact_content))
    
    # Podsumowanie
    print(f"\nğŸ“Š PODSUMOWANIE KORPUSU:")
    print(f"ğŸ“š Dokumenty: {corpus['metadata']['total_files']}")
    print(f"ğŸ“„ Strony: ~{corpus['metadata']['total_pages']}")
    print(f"ğŸ“ SÅ‚owa: ~{corpus['metadata']['total_words']:,}")
    print(f"ğŸ’¾ Rozmiar: {corpus['metadata']['total_size_kb']:.1f} KB")
    print(f"ğŸ·ï¸  Tematy: {', '.join(corpus['metadata']['topics'])}")
    
    print(f"\nğŸ“„ Utworzone pliki:")
    print(f"ğŸ“‹ cybernetyka_corpus_metadata.json - metadane korpusu")
    print(f"ğŸ“„ cybernetyka_corpus.txt - peÅ‚ny korpus ({len(unified_content)/1024:.1f} KB)")
    compact_size = len('\n'.join(compact_content))/1024
    print(f"ğŸ“„ cybernetyka_corpus_compact.txt - kompaktowa wersja ({compact_size:.1f} KB)")
    
    return corpus

if __name__ == "__main__":
    create_unified_corpus() 