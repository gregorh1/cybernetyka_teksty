#!/usr/bin/env python3
"""
Tworzenie korpusu cybernetyki dla OpenWebUI
ÅÄ…czy wszystkie pliki .txt w strukturalny korpus z metadanymi
"""

import os
import re
from pathlib import Path
import json
from datetime import datetime
import argparse

def extract_metadata_from_txt(txt_path):
    """WyciÄ…ga metadane z pliku tekstowego"""
    txt_path = Path(txt_path)
    
    # Extract topic from path structure TOPICS/<topic>/...
    topic_parts = txt_path.parts
    topic = 'unknown'
    if 'TOPICS' in topic_parts:
        topic_index = topic_parts.index('TOPICS')
        if topic_index + 1 < len(topic_parts):
            topic = topic_parts[topic_index + 1]
    
    metadata = {
        'filename': txt_path.name,
        'author_folder': txt_path.parent.name,
        'author': 'JÃ³zef Kossecki' if 'Kossecki' in str(txt_path) else 'Marian Mazur',
        'title': '',
        'original_file': '',
        'conversion_method': '',
        'pages': 0,
        'size_kb': round(txt_path.stat().st_size / 1024, 1),
        'words_estimate': 0,
        'topic': topic,
        'topics': []
    }
    
    # Czytaj nagÅ‚Ã³wek pliku Å¼eby wyciÄ…gnÄ…Ä‡ metadane
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            header = f.read(500)  # Pierwsze 500 znakÃ³w
            
        # WyciÄ…gnij tytuÅ‚ z pierwszej linii
        lines = header.split('\n')
        if lines and lines[0].startswith('=== '):
            metadata['title'] = lines[0].replace('=== ', '').replace(' ===', '').strip()
        
        # Szukaj informacji o oryginalnym pliku
        for line in lines[:10]:
            if 'Original PDF:' in line:
                metadata['original_file'] = line.split('Original PDF:')[1].strip()
            elif 'Converted from:' in line:
                metadata['original_file'] = line.split('Converted from:')[1].strip()
            elif 'Generated by:' in line:
                metadata['conversion_method'] = line.split('Generated by:')[1].strip()
            elif 'Total pages:' in line:
                try:
                    metadata['pages'] = int(re.search(r'\d+', line).group())
                except:
                    pass
                    
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d czytania metadanych z {txt_path}: {e}")
    
    # Zgadnij tematy na podstawie nazwy pliku
    filename_lower = txt_path.stem.lower()
    topic_keywords = {
        'cybernetyka': ['cybernetyka', 'cybernetic'],
        'sterowanie': ['sterowanie', 'sterowniczych', 'sterowania'],
        'informacja': ['informacji', 'informacyjn'],
        'spoÅ‚ecznoÅ›Ä‡': ['spoleczn', 'spoÅ‚eczn'],
        'system': ['system', 'ukÅ‚adÃ³w', 'ukladow'],
        'nauka': ['nauki', 'naukow'],
        'teoria': ['teoria', 'teorii'],
        'filozofia': ['filozofi', 'poznawcz'],
        'polityka': ['polityk', 'opozycj'],
        'cywilizacja': ['cywilizacj'],
        'charakterologia': ['charakter'],
        'edukacja': ['szkola', 'nauczania', 'dydaktyk'],
        'historia': ['histori', 'dzieje']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in filename_lower for keyword in keywords):
            metadata['topics'].append(topic)
    
    return metadata

def estimate_words(text):
    """PrzybliÅ¼ona liczba sÅ‚Ã³w w tekÅ›cie"""
    # UsuÅ„ znaczniki stron i nagÅ‚Ã³wki
    text = re.sub(r'--- Page \d+ ---', '', text)
    text = re.sub(r'=== .* ===', '', text)
    
    # Policz sÅ‚owa
    words = len(text.split())
    return words

def clean_text_content(text):
    """CzyÅ›ci tekst z artefaktÃ³w OCR"""
    # UsuÅ„ nadmiernie powtarzajÄ…ce siÄ™ znaki
    text = re.sub(r'(.)\1{5,}', r'\1\1\1', text)
    
    # Popraw typowe bÅ‚Ä™dy OCR w polskim tekÅ›cie
    ocr_fixes = {
        'Ä™': ['e', 'q'],
        'Ä…': ['a', 'q'],  
        'Å›': ['s'],
        'Ä‡': ['c'],
        'Åº': ['z'],
        'Å¼': ['z'],
        'Å„': ['n'],
        'Å‚': ['l'],
        'Ã³': ['o']
    }
    
    # UsuÅ„ dziwne znaki
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\'\/\n\r\t]', ' ', text)
    
    # Popraw wielokrotne spacje
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n +', '\n', text)
    
    return text.strip()

def get_available_topics():
    """Pobierz dostÄ™pne tematy"""
    topics_path = Path("TOPICS")
    if not topics_path.exists():
        return []
    
    topics = []
    for item in topics_path.iterdir():
        if item.is_dir():
            topics.append(item.name)
    
    return sorted(topics)

def find_txt_files(topic=None):
    """ZnajdÅº pliki .txt w okreÅ›lonym temacie lub wszystkich tematach"""
    topics_path = Path("TOPICS")
    
    if not topics_path.exists():
        print("âŒ Folder TOPICS/ nie istnieje")
        return []
    
    txt_files = []
    
    if topic:
        # Szukaj tylko w okreÅ›lonym temacie
        topic_path = topics_path / topic
        if not topic_path.exists():
            print(f"âŒ Temat '{topic}' nie istnieje")
            available_topics = get_available_topics()
            if available_topics:
                print(f"DostÄ™pne tematy: {', '.join(available_topics)}")
            return []
        
        txt_files.extend(list(topic_path.rglob('*.txt')))
        print(f"ğŸ” Szukanie plikÃ³w .txt w temacie: {topic}")
    else:
        # Szukaj we wszystkich tematach
        txt_files.extend(list(topics_path.rglob('*.txt')))
        print(f"ğŸ” Szukanie plikÃ³w .txt we wszystkich tematach")
    
    return txt_files

def create_unified_corpus(topic=None):
    """Tworzy jednolity korpus ze wszystkich plikÃ³w .txt"""
    
    if topic:
        print(f"ğŸ“š Tworzenie korpusu dla tematu: {topic}")
    else:
        print("ğŸ“š Tworzenie korpusu ze wszystkich tematÃ³w...")
    print("=" * 50)
    
    # ZnajdÅº wszystkie pliki .txt
    txt_files = find_txt_files(topic)
    
    if not txt_files:
        if topic:
            print(f"âŒ Nie znaleziono plikÃ³w .txt w temacie '{topic}'")
        else:
            print("âŒ Nie znaleziono plikÃ³w .txt")
        return
    
    print(f"ğŸ“ Znaleziono {len(txt_files)} plikÃ³w .txt")
    
    # Przygotuj strukturÄ™ korpusu
    corpus_title = f"Korpus {topic.title()}" if topic else "Korpus Cybernetyki Polskiej"
    corpus_description = f"ZbiÃ³r tekstÃ³w z tematu: {topic}" if topic else "ZbiÃ³r tekstÃ³w z zakresu cybernetyki spoÅ‚ecznej i ogÃ³lnej autorstwa JÃ³zefa Kosseckiego i Mariana Mazura"
    
    corpus = {
        'metadata': {
            'title': corpus_title,
            'description': corpus_description,
            'topic_filter': topic,
            'authors': ['JÃ³zef Kossecki', 'Marian Mazur'],
            'created': datetime.now().isoformat(),
            'total_files': len(txt_files),
            'total_size_kb': 0,
            'total_words': 0,
            'total_pages': 0,
            'topics': set()
        },
        'documents': []
    }
    
    unified_text = []
    unified_text.append(corpus_title.upper())
    unified_text.append("="*50)
    unified_text.append(corpus_description)
    if not topic:
        unified_text.append(f"Autorzy: JÃ³zef Kossecki, Marian Mazur")
    unified_text.append(f"Utworzony: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    unified_text.append(f"Liczba dokumentÃ³w: {len(txt_files)}")
    unified_text.append("")
    
    # PrzetwÃ³rz kaÅ¼dy plik
    for txt_file in sorted(txt_files):
        print(f"ğŸ”„ Przetwarzam: {txt_file.name}")
        
        try:
            # WyciÄ…gnij metadane
            metadata = extract_metadata_from_txt(txt_file)
            
            # Czytaj treÅ›Ä‡
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # WyczyÅ›Ä‡ tekst
            cleaned_content = clean_text_content(content)
            
            # Oszacuj liczbÄ™ sÅ‚Ã³w
            word_count = estimate_words(cleaned_content)
            metadata['words_estimate'] = word_count
            
            # Dodaj do korpusu
            corpus['documents'].append({
                'metadata': metadata,
                'content': cleaned_content[:2000] + '...' if len(cleaned_content) > 2000 else cleaned_content  # Fragment dla JSON
            })
            
            # Dodaj do zunifikowanego tekstu
            unified_text.append(f"\n{'='*80}")
            unified_text.append(f"DOKUMENT: {metadata['title'] or txt_file.stem}")
            unified_text.append(f"AUTOR: {metadata['author']}")
            unified_text.append(f"PLIK: {metadata['filename']}")
            unified_text.append(f"ROZMIAR: {metadata['size_kb']} KB")
            unified_text.append(f"SÅOWA: ~{word_count}")
            if metadata['topics']:
                unified_text.append(f"TEMATY: {', '.join(metadata['topics'])}")
            unified_text.append(f"{'='*80}")
            unified_text.append("")
            unified_text.append(cleaned_content)
            unified_text.append("")
            
            # Aktualizuj statystyki korpusu
            corpus['metadata']['total_size_kb'] += metadata['size_kb']
            corpus['metadata']['total_words'] += word_count
            corpus['metadata']['total_pages'] += metadata['pages']
            corpus['metadata']['topics'].update(metadata['topics'])
            
            print(f"âœ… Dodano: {metadata['title'] or txt_file.stem} ({word_count} sÅ‚Ã³w)")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przetwarzania {txt_file}: {e}")
    
    # Konwertuj set na listÄ™ dla JSON
    corpus['metadata']['topics'] = list(corpus['metadata']['topics'])
    
    # OkreÅ›l nazwy plikÃ³w wyjÅ›ciowych i lokalizacjÄ™
    if topic:
        # Zapisz w folderze tematu
        topic_path = Path("TOPICS") / topic
        metadata_file = topic_path / 'corpus_metadata.json'
        corpus_file = topic_path / 'corpus.txt'
        compact_file = topic_path / 'corpus_compact.txt'
    else:
        # Gdy nie ma okreÅ›lonego tematu, zapisz w katalogu gÅ‚Ã³wnym
        metadata_file = 'cybernetyka_corpus_metadata.json'
        corpus_file = 'cybernetyka_corpus.txt'
        compact_file = 'cybernetyka_corpus_compact.txt'
    
    # Zapisz metadane korpusu
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(corpus, f, indent=2, ensure_ascii=False)
    
    # Zapisz zunifikowany korpus tekstowy
    unified_content = '\n'.join(unified_text)
    with open(corpus_file, 'w', encoding='utf-8') as f:
        f.write(unified_content)
    
    # Zapisz takÅ¼e kompaktowÄ… wersjÄ™ dla OpenWebUI
    compact_content = []
    if topic:
        compact_content.append(f"{topic.upper()} - KORPUS TEKSTÃ“W")
    else:
        compact_content.append("CYBERNETYKA POLSKA - KORPUS TEKSTÃ“W")
        compact_content.append("JÃ³zef Kossecki & Marian Mazur")
    compact_content.append("")
    
    for doc in corpus['documents']:
        meta = doc['metadata']
        compact_content.append(f"\n## {meta['title'] or meta['filename']}")
        compact_content.append(f"**Autor:** {meta['author']}")
        if meta.get('topic'):
            compact_content.append(f"**Temat:** {meta['topic']}")
        if meta['topics']:
            compact_content.append(f"**Tematy:** {', '.join(meta['topics'])}")
        compact_content.append("")
        compact_content.append(doc['content'][:1500] + "..." if len(doc['content']) > 1500 else doc['content'])
    
    with open(compact_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(compact_content))
    
    # Podsumowanie
    print(f"\nğŸ“Š PODSUMOWANIE KORPUSU:")
    print(f"ğŸ“š Dokumenty: {corpus['metadata']['total_files']}")
    print(f"ğŸ“„ Strony: ~{corpus['metadata']['total_pages']}")
    print(f"ğŸ“ SÅ‚owa: ~{corpus['metadata']['total_words']:,}")
    print(f"ğŸ’¾ Rozmiar: {corpus['metadata']['total_size_kb']:.1f} KB")
    print(f"ğŸ·ï¸  Tematy: {', '.join(corpus['metadata']['topics'])}")
    
    print(f"\nğŸ“„ Utworzone pliki:")
    print(f"ğŸ“‹ {metadata_file} - metadane korpusu")
    print(f"ğŸ“„ {corpus_file} - peÅ‚ny korpus ({len(unified_content)/1024:.1f} KB)")
    compact_size = len('\n'.join(compact_content))/1024
    print(f"ğŸ“„ {compact_file} - kompaktowa wersja ({compact_size:.1f} KB)")
    
    return corpus

def build_all_topic_corpuses():
    """Zbuduj korpus dla kaÅ¼dego tematu oddzielnie"""
    topics = get_available_topics()
    
    if not topics:
        print("âŒ Nie znaleziono tematÃ³w w folderze TOPICS/")
        return
    
    print(f"ğŸ“š Tworzenie korpusÃ³w dla wszystkich tematÃ³w ({len(topics)} tematÃ³w)")
    print("=" * 60)
    
    for i, topic in enumerate(topics, 1):
        print(f"\nğŸ”„ [{i}/{len(topics)}] Przetwarzam temat: {topic}")
        print("-" * 40)
        
        try:
            create_unified_corpus(topic)
            print(f"âœ… Korpus dla tematu '{topic}' zostaÅ‚ utworzony")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas tworzenia korpusu dla tematu '{topic}': {e}")
    
    print(f"\nğŸ‰ ZakoÅ„czono tworzenie korpusÃ³w dla wszystkich tematÃ³w!")
    print(f"ğŸ“ KaÅ¼dy temat ma swÃ³j korpus w folderze TOPICS/<temat>/")

def main():
    """GÅ‚Ã³wna funkcja z obsÅ‚ugÄ… argumentÃ³w wiersza poleceÅ„"""
    parser = argparse.ArgumentParser(description="Tworzenie korpusu tekstÃ³w z plikÃ³w .txt")
    parser.add_argument("-t", "--topic", help="Przetwarzaj tylko pliki z okreÅ›lonego tematu")
    parser.add_argument("--list-topics", action="store_true", help="PokaÅ¼ dostÄ™pne tematy")
    parser.add_argument("--all-topics", action="store_true", help="Zbuduj korpus dla kaÅ¼dego tematu oddzielnie")
    
    args = parser.parse_args()
    
    if args.list_topics:
        topics = get_available_topics()
        if topics:
            print("ğŸ“‚ DostÄ™pne tematy:")
            for topic in topics:
                print(f"  â€¢ {topic}")
        else:
            print("âŒ Nie znaleziono tematÃ³w w folderze TOPICS/")
        return
    
    if args.all_topics:
        build_all_topic_corpuses()
        return
    
    create_unified_corpus(args.topic)

if __name__ == "__main__":
    main() 