# üì¶ TRANSFER NA MASZYNƒò Z GPU - INSTRUKCJE

Przewodnik przeniesienia korpusu cybernetyki i uruchomienia fine-tuningu na maszynie z RTX 3090.

## üìã PLIKI DO PRZENIESIENIA

### ‚úÖ WYMAGANE (MINIMUM):
```bash
cybernetyka_corpus.txt           # 11MB - korpus treningowy
setup_environment.py             # Setup ≈õrodowiska
finetune_bielik.py              # Skrypt fine-tuningu  
test_finetuned_model.py         # Testy modelu
run_finetune.sh                 # Automatyczny launcher
README_FINETUNE.md              # Dokumentacja
```

### üìÅ OPCJONALNE (dla pe≈Çnego archiwum):
```bash
Kossecki/                       # Wszystkie pliki ≈∫r√≥d≈Çowe
Mazur/                          
cybernetyka_corpus_metadata.json
*.py                           # Wszystkie skrypty
```

## üöÄ SZYBKI START na nowej maszynie

### 1. Skopiuj pliki
```bash
# Utw√≥rz folder projektu
mkdir cybernetyka-finetune
cd cybernetyka-finetune

# Skopiuj wymagane pliki (scp, rsync, USB, itp.)
# Minimum: korpus + skrypty Python + launcher
```

### 2. Setup ≈õrodowiska
```bash
# Stw√≥rz pyenv environment
pyenv install 3.11.7  # je≈õli nie ma
pyenv virtualenv 3.11.7 cybernetyka-finetune
pyenv activate cybernetyka-finetune

# ALBO u≈ºyj virtualenv/conda
python3 -m venv cybernetyka-env
source cybernetyka-env/bin/activate
```

### 3. Automatyczny setup i fine-tuning
```bash
# OPCJA A: Wszystko automatycznie
chmod +x run_finetune.sh
./run_finetune.sh

# OPCJA B: Krok po kroku
python3 setup_environment.py        # Setup (5-15 min)
python3 finetune_bielik.py          # Fine-tuning (2-6h)
python3 test_finetuned_model.py     # Test modelu
```

## ‚öôÔ∏è WYMAGANIA SPRZƒòTOWE

### MINIMUM:
- **GPU:** RTX 3090 24GB ‚úÖ
- **RAM:** 16GB+ 
- **Dysk:** 50GB wolne
- **CUDA:** 11.8+ lub 12.1+

### ZALECANE:
- **RAM:** 32GB+
- **Dysk:** SSD + 100GB 
- **CPU:** 8+ cores

## üîß TROUBLESHOOTING

### Problem: CUDA not available
```bash
# Sprawd≈∫ sterowniki
nvidia-smi

# Zainstaluj CUDA (Ubuntu/Debian)
sudo apt update
sudo apt install nvidia-cuda-toolkit

# Arch Linux
sudo pacman -S cuda
```

### Problem: PyTorch bez CUDA
```bash
# Odinstaluj i zainstaluj z CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Problem: Out of Memory
```bash
# Edytuj finetune_bielik.py:
"per_device_train_batch_size": 1    # Zmniejsz z 2 do 1
"max_seq_length": 1024              # Zmniejsz z 2048
```

### Problem: B≈Çƒôdy instalacji Unsloth
```bash
# Alternatywa 1:
pip install "unsloth @ git+https://github.com/unslothai/unsloth.git"

# Alternatywa 2: 
conda install unsloth -c conda-forge

# Alternatywa 3: bez Unsloth (wolniej)
# Zakomentuj Unsloth imports, u≈ºyj standardowego HuggingFace
```

## üìä MONITORING

### Podczas trenowania:
```bash
# Terminal 1: Trenowanie
python3 finetune_bielik.py

# Terminal 2: Monitor GPU
watch nvidia-smi

# Terminal 3: Logi
tail -f outputs/training.log
```

### Metryki sukcesu:
- **GPU Utilization:** 90%+
- **VRAM Usage:** ~16GB/24GB
- **Training Loss:** spada z ~2.0 do ~0.8
- **Temperature:** <85¬∞C

## üìÅ STRUKTURA WYNIK√ìW

Po fine-tuningu otrzymasz:
```
cybernetyka-finetune/
‚îú‚îÄ‚îÄ bielik-cybernetyka-lora/        # LoRA model (~1GB)
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer_config.json
‚îú‚îÄ‚îÄ bielik-cybernetyka/             # GGUF model dla Ollama
‚îÇ   ‚îî‚îÄ‚îÄ model-q4_k_m.gguf
‚îú‚îÄ‚îÄ outputs/                        # Logi i checkpointy
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-100/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-200/
‚îÇ   ‚îî‚îÄ‚îÄ training.log
‚îî‚îÄ‚îÄ requirements.txt                # Lista pakiet√≥w
```

## üéØ SPRAWDZENIE JAKO≈öCI

### Test podstawowy:
```python
python3 test_finetuned_model.py
```

### Test manualny:
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained("./bielik-cybernetyka-lora")
FastLanguageModel.for_inference(model)

# Zadaj pytanie o cybernetykƒô...
```

### Oczekiwane wyniki:
- ‚úÖ Odpowiada po polsku
- ‚úÖ U≈ºywa terminologii cybernetycznej  
- ‚úÖ Odwo≈Çuje siƒô do Kosseckiego/Mazura
- ‚úÖ Sp√≥jne z polskƒÖ szko≈ÇƒÖ cybernetycznƒÖ

## üöÄ U≈ªYCIE MODELU

### W kodzie Python:
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained("./bielik-cybernetyka-lora")
# ... generuj odpowiedzi
```

### W Ollama:
```bash
ollama create bielik-cybernetyka -f bielik-cybernetyka/model-q4_k_m.gguf
ollama run bielik-cybernetyka "Wyja≈õnij teoriƒô uk≈Çad√≥w samodzielnych"
```

### W OpenWebUI + lokalny model:
1. Import modelu do Ollama
2. Wybierz w OpenWebUI
3. Dodaj RAG z korpusem

## üí° OPTYMALIZACJE

### Dla lepszej jako≈õci:
```python
# W finetune_bielik.py zmie≈Ñ:
"num_train_epochs": 3              # Wiƒôcej epochs
"r": 32                           # Wiƒôkszy LoRA rank  
"learning_rate": 1e-4             # Mniejszy LR
```

### Dla szybszego trenowania:
```python
"per_device_train_batch_size": 4   # Wiƒôkszy batch
"max_seq_length": 1024            # Kr√≥tsze sekwencje
"num_train_epochs": 1             # Mniej epochs
```

---

## üéâ SUKCES!

Po zako≈Ñczeniu bƒôdziesz mia≈Ç:
- ‚úÖ Specjalistyczny polski model cybernetyki
- ‚úÖ Wiedzƒô z 66 dokument√≥w (1.55M s≈Ç√≥w)
- ‚úÖ Ekspert w pracach Kosseckiego i Mazura
- ‚úÖ Model gotowy do produkcji

**Czas trenowania na RTX 3090: 2-6 godzin**  
**Ko≈Ñcowy rozmiar: ~2GB (LoRA) + ~6GB (GGUF)**

üí™ **Pierwszy polski specjalistyczny model AI w cybernetyce!** üáµüá±ü§ñ 