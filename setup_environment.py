#!/usr/bin/env python3
"""
Setup Environment dla Fine-tuningu Bielik-11B
Sprawdza i instaluje wszystkie wymagane biblioteki
"""

import sys
import subprocess
import importlib
import platform

def check_python_version():
    """SprawdÅº wersjÄ™ Python"""
    print("ğŸ Sprawdzam wersjÄ™ Python...")
    
    version = sys.version_info
    if version.major < 3 or (version.major == 3 and version.minor < 10):
        print(f"âŒ Python {version.major}.{version.minor} jest za stary!")
        print("   Wymagany Python 3.10+")
        return False
    
    print(f"âœ… Python {version.major}.{version.minor}.{version.micro} - OK")
    return True

def check_cuda():
    """SprawdÅº dostÄ™pnoÅ›Ä‡ CUDA"""
    print("ğŸ® Sprawdzam CUDA...")
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIA GPU wykryte")
            return True
        else:
            print("âŒ nvidia-smi nie dziaÅ‚a")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smi nie znalezione - brak NVIDIA GPU lub sterownikÃ³w")
        return False

def check_pytorch_cuda():
    """SprawdÅº czy PyTorch ma obsÅ‚ugÄ™ CUDA"""
    print("ğŸ”¥ Sprawdzam PyTorch z CUDA...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            print(f"âœ… PyTorch z CUDA - OK")
            print(f"   GPU: {gpu_name}")
            print(f"   VRAM: {gpu_memory:.1f} GB")
            print(f"   Count: {gpu_count}")
            
            if gpu_memory < 20:
                print("âš ï¸  VRAM < 20GB - uÅ¼yj QLoRA zamiast LoRA")
            
            return True
        else:
            print("âŒ PyTorch bez obsÅ‚ugi CUDA")
            return False
            
    except ImportError:
        print("âš ï¸  PyTorch nie zainstalowany - bÄ™dzie zainstalowany")
        return False

def install_package(package, check_name=None):
    """Zainstaluj pakiet pip"""
    check_name = check_name or package.split('[')[0].split('@')[0].split('==')[0]
    
    try:
        importlib.import_module(check_name.replace('-', '_'))
        print(f"âœ… {check_name} juÅ¼ zainstalowany")
        return True
    except ImportError:
        print(f"ğŸ“¦ InstalujÄ™ {package}...")
        
        try:
            if '@' in package or 'git+' in package:
                # Git packages
                cmd = f"pip install '{package}'"
            else:
                cmd = f"pip install {package}"
                
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… {package} zainstalowany pomyÅ›lnie")
                return True
            else:
                print(f"âŒ BÅ‚Ä…d instalacji {package}:")
                print(result.stderr)
                return False
                
        except Exception as e:
            print(f"âŒ WyjÄ…tek podczas instalacji {package}: {e}")
            return False

def install_pytorch_cuda():
    """Zainstaluj PyTorch z obsÅ‚ugÄ… CUDA"""
    print("ğŸ”¥ InstalujÄ™ PyTorch z CUDA...")
    
    # SprawdÅº aktualnÄ… wersjÄ™ CUDA
    cuda_version = "cu121"  # Default dla nowych systemÃ³w
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if "CUDA Version: 12.1" in result.stdout or "CUDA Version: 12.2" in result.stdout:
            cuda_version = "cu121"
        elif "CUDA Version: 11.8" in result.stdout:
            cuda_version = "cu118"
    except:
        pass
    
    pytorch_cmd = f"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/{cuda_version}"
    
    print(f"ğŸ“¦ InstalujÄ™ PyTorch dla {cuda_version}...")
    result = subprocess.run(pytorch_cmd, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… PyTorch z CUDA zainstalowany")
        return True
    else:
        print("âŒ BÅ‚Ä…d instalacji PyTorch:")
        print(result.stderr)
        return False

def main():
    """GÅ‚Ã³wna funkcja setup"""
    
    print("ğŸš€ SETUP ENVIRONMENT - Fine-tuning Bielik-11B")
    print("=" * 60)
    
    # Lista sprawdzeÅ„
    checks = []
    
    # SprawdÅº Python
    checks.append(check_python_version())
    
    # SprawdÅº CUDA
    checks.append(check_cuda())
    
    # SprawdÅº PyTorch
    pytorch_ok = check_pytorch_cuda()
    if not pytorch_ok:
        pytorch_ok = install_pytorch_cuda()
        if pytorch_ok:
            pytorch_ok = check_pytorch_cuda()
    checks.append(pytorch_ok)
    
    if not all(checks[:3]):
        print("\nâŒ Podstawowe wymagania nie sÄ… speÅ‚nione!")
        print("   Napraw bÅ‚Ä™dy przed kontynuowaniem.")
        return False
    
    print("\nğŸ“¦ InstalujÄ™ biblioteki dla fine-tuningu...")
    
    # Lista pakietÃ³w do zainstalowania
    packages = [
        # Core packages
        ("datasets", "datasets"),
        ("transformers>=4.36.0", "transformers"),
        ("accelerate", "accelerate"),
        ("peft", "peft"),
        ("trl", "trl"),
        ("bitsandbytes", "bitsandbytes"),
        
        # Unsloth (najwaÅ¼niejsze)
        ("unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git", "unsloth"),
        
        # Optional but recommended
        ("xformers", "xformers"),
        ("flash-attn --no-build-isolation", "flash_attn"),
    ]
    
    success_count = 0
    for package, check_name in packages:
        if install_package(package, check_name):
            success_count += 1
        else:
            print(f"âš ï¸  {package} nie zostaÅ‚ zainstalowany - moÅ¼e dziaÅ‚aÄ‡ bez tego")
    
    print(f"\nğŸ“Š PODSUMOWANIE INSTALACJI:")
    print(f"âœ… Zainstalowane: {success_count}/{len(packages)} pakietÃ³w")
    
    # Test finalny
    print(f"\nğŸ§ª FINAL TEST...")
    try:
        import torch
        from transformers import AutoTokenizer
        import datasets
        
        if torch.cuda.is_available():
            print("âœ… Åšrodowisko gotowe do fine-tuningu!")
            
            # SprawdÅº Unsloth
            try:
                from unsloth import FastLanguageModel
                print("âœ… Unsloth zainstalowany i gotowy")
            except ImportError:
                print("âš ï¸  Unsloth niedostÄ™pny - sprÃ³buj ponownie")
            
            return True
        else:
            print("âŒ CUDA niedostÄ™pne w PyTorch")
            return False
            
    except ImportError as e:
        print(f"âŒ Brak wymaganych bibliotek: {e}")
        return False

def create_requirements_file():
    """UtwÃ³rz plik requirements.txt"""
    requirements = """# Fine-tuning Bielik-11B Requirements
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.6.0
trl>=0.7.0
bitsandbytes>=0.41.0
xformers>=0.0.22
# Unsloth - install separately with git+
# unsloth @ git+https://github.com/unslothai/unsloth.git
"""
    
    with open("requirements.txt", "w") as f:
        f.write(requirements)
    
    print("ğŸ“„ Utworzono requirements.txt")

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸ‰ ENVIRONMENT GOTOWY!")
        print(f"   MoÅ¼esz uruchomiÄ‡: python3 finetune_bielik.py")
        create_requirements_file()
    else:
        print(f"\nâŒ ENVIRONMENT NIE GOTOWY")
        print(f"   Napraw bÅ‚Ä™dy i uruchom ponownie")
        
    print(f"\nğŸ’¡ Pro Tips:")
    print(f"   - Monitor GPU: watch nvidia-smi")
    print(f"   - Logi trenowania: tail -f outputs/training.log") 
    print(f"   - Zmniejsz batch_size jeÅ›li OOM error") 