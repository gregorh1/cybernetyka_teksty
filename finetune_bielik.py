#!/usr/bin/env python3
"""
Fine-tuning Bielik-11B z korpusem cybernetyki przy uÅ¼yciu Unsloth
Optimized dla RTX 3090 (24GB VRAM)
"""

import torch
from datasets import Dataset
import json
from pathlib import Path
import re

def check_environment():
    """SprawdÅº czy Å›rodowisko jest gotowe"""
    print("ğŸ”§ Sprawdzam Å›rodowisko...")
    
    # SprawdÅº CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDA nie jest dostÄ™pne!")
        print("   Uruchom setup_environment.py aby skonfigurowaÄ‡")
        return False
    
    # SprawdÅº wymagane biblioteki
    required_modules = ['unsloth', 'transformers', 'datasets', 'peft', 'trl']
    missing = []
    
    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing.append(module)
    
    if missing:
        print(f"âŒ BrakujÄ…ce biblioteki: {', '.join(missing)}")
        print("   Uruchom: python3 setup_environment.py")
        return False
    
    print("âœ… Åšrodowisko gotowe do fine-tuningu")
    return True

def prepare_corpus_for_training(corpus_file="cybernetyka_corpus.txt", max_length=2048):
    """Przygotuj korpus do trenowania"""
    
    print("ğŸ“š PrzygotowujÄ™ korpus do trenowania...")
    
    with open(corpus_file, 'r', encoding='utf-8') as f:
        full_text = f.read()
    
    # Podziel na dokumenty
    documents = re.split(r'={80}', full_text)
    
    training_data = []
    
    for i, doc in enumerate(documents):
        if len(doc.strip()) < 100:  # PomiÅ„ puste sekcje
            continue
            
        # WyczyÅ›Ä‡ dokument
        doc = doc.strip()
        
        # UsuÅ„ metadane nagÅ‚Ã³wka jeÅ›li istniejÄ…
        lines = doc.split('\n')
        content_start = 0
        for j, line in enumerate(lines):
            if 'DOKUMENT:' in line or 'AUTOR:' in line:
                content_start = j + 6  # PomiÅ„ metadane
                break
        
        if content_start > 0:
            doc = '\n'.join(lines[content_start:])
        
        # Podziel na fragmenty o maksymalnej dÅ‚ugoÅ›ci
        chunks = split_text_smart(doc, max_length)
        
        for chunk in chunks:
            if len(chunk.strip()) > 50:  # PomiÅ„ za krÃ³tkie fragmenty
                training_data.append({
                    "text": chunk.strip(),
                    "input": "",  # Dla continued pre-training
                    "output": chunk.strip()
                })
    
    print(f"âœ… Przygotowano {len(training_data)} fragmentÃ³w do trenowania")
    return training_data

def split_text_smart(text, max_length=2048):
    """Inteligentny podziaÅ‚ tekstu na fragmenty"""
    
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences = re.split(r'[.!?]\s+', text)
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 2 <= max_length:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def create_instruction_format(data):
    """Konwertuj do formatu instrukcyjnego dla Bielik"""
    
    formatted_data = []
    
    for item in data:
        text = item["text"]
        
        # Format dla Bielik (podobny do ChatML)
        formatted_text = f"""<|start_header_id|>system<|end_header_id|>

JesteÅ› ekspertem z zakresu cybernetyki spoÅ‚ecznej i ogÃ³lnej. Odpowiadasz na podstawie polskiej szkoÅ‚y cybernetycznej, w szczegÃ³lnoÅ›ci prac JÃ³zefa Kosseckiego i Mariana Mazura.<|eot_id|><|start_header_id|>user<|end_header_id|>

Kontynuuj lub wyjaÅ›nij nastÄ™pujÄ…cy tekst z zakresu cybernetyki: {text[:200]}...<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{text}<|eot_id|>"""
        
        formatted_data.append({
            "text": formatted_text
        })
    
    return formatted_data

def main():
    """GÅ‚Ã³wna funkcja fine-tuningu"""
    
    print("ğŸš€ Fine-tuning Bielik-11B z korpusem cybernetyki")
    print("=" * 60)
    
    # SprawdÅº Å›rodowisko
    if not check_environment():
        return
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ VRAM: {gpu_memory:.1f} GB")
    
    if gpu_memory < 20:
        print("âš ï¸  Uwaga: MaÅ‚o VRAM. UÅ¼yj QLoRA zamiast LoRA")
    
    # Import bibliotek
    from unsloth import FastLanguageModel
    
    # Przygotuj dane
    print("\nğŸ“š PrzygotowujÄ™ dane treningowe...")
    training_data = prepare_corpus_for_training()
    
    # Formatuj do instrukcji (opcjonalne)
    # training_data = create_instruction_format(training_data)
    
    # UtwÃ³rz dataset
    dataset = Dataset.from_list(training_data)
    
    print(f"ğŸ“Š Dataset: {len(dataset)} przykÅ‚adÃ³w")
    
    # Model config dla RTX 3090
    model_config = {
        "model_name": "speakleash/Bielik-11B-v2.3-Instruct",
        "max_seq_length": 2048,  # ZwiÄ™ksz jeÅ›li masz wiÄ™cej VRAM
        "dtype": None,  # Auto-detect
        "load_in_4bit": True,  # QLoRA dla oszczÄ™dnoÅ›ci VRAM
    }
    
    print(f"\nğŸ¤– ÅadujÄ™ model Bielik-11B...")
    model, tokenizer = FastLanguageModel.from_pretrained(**model_config)
    
    # Konfiguracja LoRA
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # LoRA rank - zwiÄ™ksz dla lepszej jakoÅ›ci
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ],
        lora_alpha=16,
        lora_dropout=0.0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )
    
    # Training arguments
    training_args = {
        "per_device_train_batch_size": 2,  # ZwiÄ™ksz jeÅ›li moÅ¼esz
        "gradient_accumulation_steps": 4,
        "warmup_steps": 5,
        "num_train_epochs": 1,  # ZwiÄ™ksz dla lepszych rezultatÃ³w
        "learning_rate": 2e-4,
        "fp16": not torch.cuda.is_bf16_supported(),
        "bf16": torch.cuda.is_bf16_supported(),
        "logging_steps": 1,
        "optim": "adamw_8bit",
        "weight_decay": 0.01,
        "lr_scheduler_type": "linear",
        "seed": 3407,
        "output_dir": "./outputs",
        "save_steps": 100,
        "save_total_limit": 3,
    }
    
    print(f"\nğŸ‹ï¸ Rozpoczynam trenowanie...")
    print(f"âš™ï¸  Batch size: {training_args['per_device_train_batch_size']}")
    print(f"âš™ï¸  Epochs: {training_args['num_train_epochs']}")
    print(f"âš™ï¸  Learning rate: {training_args['learning_rate']}")
    
    # Oszacuj czas
    steps_per_epoch = len(dataset) // (training_args['per_device_train_batch_size'] * training_args['gradient_accumulation_steps'])
    total_steps = steps_per_epoch * training_args['num_train_epochs']
    estimated_hours = total_steps * 0.1 / 60  # ~0.1 min per step na RTX 3090
    
    print(f"â±ï¸  Szacowany czas: {estimated_hours:.1f} godzin")
    
    # Trenowanie
    from trl import SFTTrainer
    from transformers import TrainingArguments
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=model_config["max_seq_length"],
        dataset_num_proc=2,
        packing=False,
        args=TrainingArguments(**training_args),
    )
    
    # Start training
    trainer_stats = trainer.train()
    
    # Zapisz model
    print(f"\nğŸ’¾ ZapisujÄ™ model...")
    model.save_pretrained("bielik-cybernetyka-lora")
    tokenizer.save_pretrained("bielik-cybernetyka-lora")
    
    # Zapisz jako GGUF dla Ollama (opcjonalne)
    print(f"\nğŸ“¦ EksportujÄ™ do GGUF...")
    model.save_pretrained_gguf("bielik-cybernetyka", tokenizer, quantization_method="q4_k_m")
    
    print(f"\nğŸ‰ Fine-tuning zakoÅ„czony!")
    print(f"ğŸ“‚ Model zapisany w: ./bielik-cybernetyka-lora")
    print(f"ğŸ“‚ GGUF zapisany w: ./bielik-cybernetyka")
    
    return trainer_stats

if __name__ == "__main__":
    # Uruchom fine-tuning
    stats = main()
    
    print(f"\nğŸ“Š STATYSTYKI TRENOWANIA:")
    print(f"â±ï¸  Czas trenowania: {stats.metrics['train_runtime']:.1f} sekund")
    print(f"ğŸ“ˆ Loss: {stats.metrics['train_loss']:.4f}")
    print(f"ğŸš€ Samples/second: {stats.metrics['train_samples_per_second']:.2f}") 