#!/usr/bin/env python3
"""
Automatyczne tworzenie datasetu Q&A z korpusu cybernetyki
ObsÅ‚uguje: OpenAI API, local models, oraz Claude API
"""

import json
import re
from pathlib import Path
import time
from typing import List, Dict, Optional
import argparse
import os

# Load .env file if present
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ZaÅ‚adowano zmienne z .env")
except ImportError:
    print("ğŸ“ Zainstaluj python-dotenv dla obsÅ‚ugi .env: pip install python-dotenv")
except Exception as e:
    print(f"âš ï¸  BÅ‚Ä…d Å‚adowania .env: {e}")

def load_corpus(corpus_file: str = "cybernetyka_corpus.txt") -> List[str]:
    """ZaÅ‚aduj i podziel korpus na fragmenty"""
    print("ğŸ“š ÅadujÄ™ korpus...")
    
    with open(corpus_file, 'r', encoding='utf-8') as f:
        full_text = f.read()
    
    # Podziel na dokumenty
    documents = re.split(r'={80}', full_text)
    documents = [doc.strip() for doc in documents if len(doc.strip()) > 200]
    
    # Podziel dÅ‚ugie dokumenty na fragmenty ~500-1000 sÅ‚Ã³w
    chunks = []
    for doc in documents:
        paragraphs = doc.split('\n\n')
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            test_chunk = current_chunk + "\n\n" + para if current_chunk else para
            
            if len(test_chunk.split()) <= 800:  # ~800 words
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    print(f"âœ… Przygotowano {len(chunks)} fragmentÃ³w do przetworzenia")
    return chunks

def generate_qa_openai(text_chunk: str, api_key: str) -> List[Dict]:
    """Generuj Q&A uÅ¼ywajÄ…c OpenAI API"""
    try:
        import openai
        openai.api_key = api_key
        
        prompt = f"""Przeczytaj poniÅ¼szy tekst z polskiej cybernetyki i wygeneruj 3-5 par pytanie-odpowiedÅº.

WYMAGANIA:
- Pytania rÃ³Å¼nego typu: faktyczne, koncepcyjne, analityczne
- Odpowiedzi oparte na tekÅ›cie, precyzyjne
- UÅ¼ywaj polskiej terminologii cybernetycznej
- Zachowaj akademicki styl

TEKST:
{text_chunk}

FORMAT (zwrÃ³Ä‡ TYLKO JSON):
[
  {{"question": "pytanie", "answer": "odpowiedÅº"}},
  {{"question": "pytanie", "answer": "odpowiedÅº"}}
]"""

        response = openai.chat.completions.create(
            model="gpt-4o-mini",  # Cheaper option
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2000
        )
        
        content = response.choices[0].message.content.strip()
        
        # Extract JSON from response
        if content.startswith('```json'):
            content = content[7:-3]
        elif content.startswith('```'):
            content = content[3:-3]
            
        return json.loads(content)
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d OpenAI API: {e}")
        return []

def generate_qa_local(text_chunk: str, model_name: str = "microsoft/Phi-3-mini-4k-instruct") -> List[Dict]:
    """Generuj Q&A uÅ¼ywajÄ…c lokalnego modelu"""
    try:
        from transformers import pipeline
        
        generator = pipeline(
            "text-generation",
            model=model_name,
            device_map="auto",
            torch_dtype="auto"
        )
        
        prompt = f"""<|user|>
Wygeneruj 3-4 pytania i odpowiedzi z tego tekstu o cybernetyce polskiej. ZwrÃ³Ä‡ w formacie JSON:

{text_chunk[:1500]}

<|assistant|>
Oto pytania i odpowiedzi w formacie JSON:
["""

        result = generator(
            prompt, 
            max_new_tokens=1000,
            temperature=0.7,
            do_sample=True,
            pad_token_id=generator.tokenizer.eos_token_id
        )
        
        generated = result[0]['generated_text'][len(prompt):]
        
        # Try to extract JSON
        try:
            # Find JSON in generated text
            json_start = generated.find('[')
            json_end = generated.rfind(']') + 1
            if json_start != -1 and json_end != -1:
                json_str = generated[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
            
        # Fallback: manual parsing
        qa_pairs = []
        lines = generated.split('\n')
        current_q = None
        
        for line in lines:
            if line.strip().startswith('Q:') or 'question' in line.lower():
                current_q = line.strip()
            elif line.strip().startswith('A:') or 'answer' in line.lower():
                if current_q:
                    qa_pairs.append({
                        "question": current_q.replace('Q:', '').strip(),
                        "answer": line.strip().replace('A:', '').strip()
                    })
                    current_q = None
                    
        return qa_pairs[:4]  # Max 4
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d lokalnego modelu: {e}")
        return []

def generate_qa_claude(text_chunk: str, api_key: str) -> List[Dict]:
    """Generuj Q&A uÅ¼ywajÄ…c Claude API"""
    try:
        import anthropic
        
        client = anthropic.Anthropic(api_key=api_key)
        
        prompt = f"""Przeczytaj ten tekst z polskiej cybernetyki i wygeneruj 3-5 par pytanie-odpowiedÅº.

TEKST:
{text_chunk}

ZwrÃ³Ä‡ odpowiedÅº w formacie JSON:
[
  {{"question": "pytanie", "answer": "odpowiedÅº"}},
  ...
]"""

        message = client.messages.create(
            model="claude-3-haiku-20240307",  # Cheapest option
            max_tokens=2000,
            temperature=0.7,
            messages=[{"role": "user", "content": prompt}]
        )
        
        content = message.content[0].text.strip()
        
        # Extract JSON
        if content.startswith('```json'):
            content = content[7:-3]
        elif content.startswith('```'):
            content = content[3:-3]
            
        return json.loads(content)
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d Claude API: {e}")
        return []

def generate_qa_ollama(text_chunk: str, model_name: str = "phi3:mini") -> List[Dict]:
    """Generuj Q&A uÅ¼ywajÄ…c Ollama"""
    try:
        import requests
        
        prompt = f"""Na podstawie tego tekstu z cybernetyki polskiej wygeneruj 3-4 pytania i odpowiedzi w formacie JSON:

{text_chunk[:1500]}

Format JSON:
[
  {{"question": "pytanie", "answer": "odpowiedÅº"}},
  ...
]"""

        # Call Ollama API
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 1000
                }
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("response", "")
            
            # Try to extract JSON
            try:
                json_start = generated_text.find('[')
                json_end = generated_text.rfind(']') + 1
                if json_start != -1 and json_end != -1:
                    json_str = generated_text[json_start:json_end]
                    qa_pairs = json.loads(json_str)
                    return qa_pairs[:4]
            except:
                pass
                
        print(f"âš ï¸  Ollama response parsing failed")
        return []
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d Ollama: {e}")
        print(f"ğŸ’¡ SprawdÅº czy Ollama dziaÅ‚a: ollama list")
        return []

def generate_qa_finetuned(text_chunk: str, model_path: str) -> List[Dict]:
    """Generuj Q&A uÅ¼ywajÄ…c wytrenowanego modelu"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        import torch
        
        print(f"ğŸ”„ ÅadujÄ™ model z: {model_path}")
        
        # Load your fine-tuned model
        base_model_name = "speakleash/Bielik-11B-v2.2-Instruct"
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        
        # Load LoRA adapter
        model = PeftModel.from_pretrained(base_model, model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Set for inference
        model.eval()
        
        prompt = f"""Na podstawie tego tekstu z cybernetyki wygeneruj 3-4 pytania i odpowiedzi:

{text_chunk[:1200]}

Format:
Q: [pytanie o cybernetyce]
A: [odpowiedÅº na podstawie tekstu]

Q:"""

        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500)
        
        # Move inputs to the same device as the model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=800,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # Decode
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_response = generated_text[len(prompt):]
        
        # Parse Q&A pairs
        qa_pairs = []
        lines = generated_response.split('\n')
        current_q = None
        current_a = None
        
        for line in lines:
            line = line.strip()
            if line.startswith('Q:'):
                if current_q and current_a:
                    qa_pairs.append({
                        "question": current_q,
                        "answer": current_a
                    })
                current_q = line[2:].strip()
                current_a = None
            elif line.startswith('A:'):
                current_a = line[2:].strip()
            elif current_a and line and not line.startswith('Q:'):
                current_a += " " + line
        
        # Add last Q&A
        if current_q and current_a:
            qa_pairs.append({
                "question": current_q,
                "answer": current_a
            })
        
        print(f"ğŸ§  Wygenerowano {len(qa_pairs)} par Q&A ekspertem cybernetyki")
        return qa_pairs[:4]
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d fine-tuned model: {e}")
        return []

def create_instruction_format(qa_pairs: List[Dict]) -> List[Dict]:
    """Przekonwertuj Q&A na format instrukcyjny"""
    instructions = []
    
    for qa in qa_pairs:
        instructions.append({
            "instruction": qa["question"],
            "input": "",
            "output": qa["answer"]
        })
        
        # Dodaj warianty
        instructions.append({
            "instruction": "Odpowiedz na pytanie dotyczÄ…ce cybernetyki:",
            "input": qa["question"],
            "output": qa["answer"]
        })
    
    return instructions

def save_progress(qa_pairs: List[Dict], output_file: str, chunk_index: int):
    """Zapisz postÄ™p po kaÅ¼dym fragmencie"""
    progress_file = output_file.replace('.json', f'_progress.json')
    backup_file = output_file.replace('.json', f'_backup_{chunk_index}.json')
    
    progress_data = {
        "last_processed_chunk": chunk_index,
        "total_qa_pairs": len(qa_pairs),
        "qa_pairs": qa_pairs,
        "timestamp": time.time()
    }
    
    # Zapisz gÅ‚Ã³wny plik postÄ™pu
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    # Zapisz backup co 10 fragmentÃ³w
    if chunk_index % 10 == 0:
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ Zapisano postÄ™p: {len(qa_pairs)} par Q&A (fragment {chunk_index})")

def load_progress(output_file: str) -> tuple:
    """ZaÅ‚aduj postÄ™p z poprzedniej sesji"""
    progress_file = output_file.replace('.json', f'_progress.json')
    
    if not Path(progress_file).exists():
        return [], 0
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        qa_pairs = progress_data.get("qa_pairs", [])
        last_chunk = progress_data.get("last_processed_chunk", 0)
        
        print(f"ğŸ“‚ Znaleziono poprzedni postÄ™p:")
        print(f"   Ostatni fragment: {last_chunk}")
        print(f"   Zebrano par Q&A: {len(qa_pairs)}")
        
        resume = input("   KontynuowaÄ‡ od tego miejsca? [T/n]: ").strip().lower()
        if resume in ['', 't', 'tak', 'y', 'yes']:
            return qa_pairs, last_chunk + 1
        else:
            return [], 0
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d wczytywania postÄ™pu: {e}")
        return [], 0

def generate_qa_with_retry(chunk: str, method: str, api_key: str = None, model: str = None, max_retries: int = 3) -> List[Dict]:
    """Generuj Q&A z ponownÄ… prÃ³bÄ… w przypadku bÅ‚Ä™du"""
    for attempt in range(max_retries):
        try:
            if method == "openai":
                return generate_qa_openai(chunk, api_key)
            elif method == "claude":
                return generate_qa_claude(chunk, api_key)
            elif method == "local":
                return generate_qa_local(chunk, model)
            elif method == "ollama":
                return generate_qa_ollama(chunk, model)
            elif method == "finetuned":
                return generate_qa_finetuned(chunk, model)
                
        except Exception as e:
            print(f"   âš ï¸  PrÃ³ba {attempt + 1}/{max_retries} nieudana: {e}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"   â³ Oczekiwanie {wait_time}s przed kolejnÄ… prÃ³bÄ…...")
                time.sleep(wait_time)
            else:
                print(f"   âŒ Wszystkie prÃ³by wyczerpane dla fragmentu")
                return []
    
    return []

def get_api_key(method: str, provided_key: str = None) -> str:
    """Pobierz API key z argumentÃ³w lub zmiennych Å›rodowiskowych"""
    if provided_key:
        return provided_key
    
    if method == "openai":
        env_key = os.getenv("OPENAI_API_KEY")
        if env_key:
            print("ğŸ”‘ UÅ¼ywam OPENAI_API_KEY z .env")
            return env_key
    elif method == "claude":
        env_key = os.getenv("ANTHROPIC_API_KEY") or os.getenv("CLAUDE_API_KEY")
        if env_key:
            print("ğŸ”‘ UÅ¼ywam ANTHROPIC_API_KEY z .env")
            return env_key
    
    return None

def show_examples():
    """PokaÅ¼ przykÅ‚ady uÅ¼ycia"""
    print("""
ğŸš€ PRZYKÅADY UÅ»YCIA create_qa_dataset.py
================================================

ğŸ¤– OPENAI (Zalecane - najwyÅ¼sza jakoÅ›Ä‡):
   python3 create_qa_dataset.py --method openai --max-chunks 10
   python3 create_qa_dataset.py --method openai --max-chunks 100 --output full_qa.json
   
   ğŸ’¡ Wymaga OPENAI_API_KEY w .env lub --api-key
   ğŸ’° Koszt: ~$0.01 za chunk (~$1 za 100 fragmentÃ³w)

ğŸ¦¾ CLAUDE (Alternatywa dla OpenAI):
   python3 create_qa_dataset.py --method claude --api-key sk-ant-your-key --max-chunks 10
   
   ğŸ’¡ Wymaga ANTHROPIC_API_KEY w .env lub --api-key
   ğŸ’° Koszt: ~$0.005 za chunk (taÅ„sze od OpenAI)

ğŸ§  FINE-TUNED MODEL (Najlepsze dla cybernetyki - po Phase 1):
   python3 create_qa_dataset.py --method finetuned --model-path ./bielik-cybernetyka-lora --max-chunks 50
   
   ğŸ’¡ Wymaga ukoÅ„czonego fine-tuningu (Phase 1)
   ğŸ’° Koszt: Darmowe!
   ğŸ¯ JakoÅ›Ä‡: DoskonaÅ‚a dla cybernetyki

ğŸ’» LOCAL (HuggingFace models - wolne, ale darmowe):
   python3 create_qa_dataset.py --method local --model microsoft/Phi-3-mini-4k-instruct --max-chunks 5
   python3 create_qa_dataset.py --method local --model mistralai/Mistral-7B-Instruct-v0.1 --max-chunks 10
   
   ğŸ’¡ Pobiera model z HuggingFace (~3-7GB)
   ğŸ’° Koszt: Darmowe (po pobraniu)

ğŸ¦™ OLLAMA (JeÅ›li masz Ollama setup):
   ollama pull phi3:mini                    # Najpierw zainstaluj model
   python3 create_qa_dataset.py --method ollama --model phi3:mini --max-chunks 10
   
   ğŸ’¡ Wymaga dziaÅ‚ajÄ…cego Ollama
   ğŸ’° Koszt: Darmowe

ğŸ”„ KONTROLA POSTÄ˜PU:
   python3 create_qa_dataset.py --method openai --resume                    # WznÃ³w przerwane
   python3 create_qa_dataset.py --method openai --save-every 5 --max-chunks 100  # Zapisuj co 5

ğŸ“Š PRZYKÅADOWE SKALE:
   --max-chunks 3      # Test (3 fragmenty, ~15 Q&A, ~$0.03)
   --max-chunks 50     # MaÅ‚y dataset (~250 Q&A, ~$0.50)
   --max-chunks 200    # Åšredni dataset (~1000 Q&A, ~$2)
   --max-chunks 2180   # PeÅ‚ny korpus (~10,000 Q&A, ~$20)

ğŸ’¡ PRO TIPS:
   - Zacznij od maÅ‚ej liczby chunks dla testÃ³w
   - OpenAI ma najwyÅ¼szÄ… jakoÅ›Ä‡
   - Fine-tuned model bÄ™dzie najlepszy dla cybernetyki
   - UÅ¼yj --resume jeÅ›li proces siÄ™ przerwie
""")

def main():
    parser = argparse.ArgumentParser(
        description="Generuj dataset Q&A z korpusu cybernetyki",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PRZYKÅADY:
  %(prog)s --method openai --max-chunks 10
  %(prog)s --method finetuned --model-path ./bielik-cybernetyka-lora --max-chunks 50
  %(prog)s --method local --model microsoft/Phi-3-mini-4k-instruct --max-chunks 5
  %(prog)s --examples  # PokaÅ¼ szczegÃ³Å‚owe przykÅ‚ady
        """)
    
    parser.add_argument("--method", choices=["openai", "claude", "local", "ollama", "finetuned"],
                       help="Metoda generowania Q&A")
    parser.add_argument("--api-key", 
                       help="API key dla OpenAI/Claude (opcjonalnie z .env)")
    parser.add_argument("--model", default="microsoft/Phi-3-mini-4k-instruct",
                       help="Model: HF model dla 'local', Ollama model dla 'ollama'")
    parser.add_argument("--model-path", 
                       help="ÅšcieÅ¼ka do fine-tuned modelu (wymagana dla --method finetuned)")
    parser.add_argument("--max-chunks", type=int, default=50,
                       help="Maksymalna liczba fragmentÃ³w do przetworzenia (default: 50)")
    parser.add_argument("--output", default="cybernetics_qa_dataset.json",
                       help="Plik wyjÅ›ciowy (default: cybernetics_qa_dataset.json)")
    parser.add_argument("--resume", action="store_true",
                       help="Automatycznie wznÃ³w z ostatniego postÄ™pu")
    parser.add_argument("--save-every", type=int, default=1,
                       help="Zapisuj postÄ™p co N fragmentÃ³w (default: 1)")
    parser.add_argument("--examples", action="store_true",
                       help="PokaÅ¼ szczegÃ³Å‚owe przykÅ‚ady uÅ¼ycia")
    
    args = parser.parse_args()
    
    # Show examples if requested
    if args.examples:
        show_examples()
        return
    
    # Validate required arguments
    if not args.method:
        parser.print_help()
        print("\nâŒ Wymagany argument --method")
        print("ğŸ’¡ UÅ¼yj --examples aby zobaczyÄ‡ przykÅ‚ady")
        return
    
    # Get API key from args or environment
    api_key = get_api_key(args.method, args.api_key)
    
    # ZaÅ‚aduj korpus
    chunks = load_corpus()
    
    if args.max_chunks:
        chunks = chunks[:args.max_chunks]
        print(f"ğŸ”¢ Ograniczenie do {len(chunks)} fragmentÃ³w")
    
    # ZaÅ‚aduj poprzedni postÄ™p
    if args.resume or Path(args.output.replace('.json', '_progress.json')).exists():
        all_qa_pairs, start_chunk = load_progress(args.output)
    else:
        all_qa_pairs = []
        start_chunk = 0
    
    # SprawdÅº wymagania dla rÃ³Å¼nych metod
    if args.method in ["openai", "claude"] and not api_key:
        print(f"âŒ Wymagany API key dla {args.method}")
        print(f"   Dodaj do .env: {'OPENAI_API_KEY' if args.method == 'openai' else 'ANTHROPIC_API_KEY'}=sk-...")
        print(f"   Lub uÅ¼yj: --api-key YOUR_KEY")
        return
    
    if args.method == "finetuned" and not args.model_path:
        print(f"âŒ Wymagana Å›cieÅ¼ka do modelu dla --method finetuned")
        print(f"   UÅ¼yj: --model-path ./bielik-cybernetyka-lora")
        return
    
    if args.method == "ollama":
        print(f"ğŸ’¡ SprawdÅº czy Ollama dziaÅ‚a: ollama list")
        print(f"   Model: {args.model}")
    
    if args.method == "finetuned":
        print(f"ğŸ§  UÅ¼ywam fine-tuned modelu z: {args.model_path}")
    
    print(f"ğŸ¤– Rozpoczynam generowanie Q&A metodÄ…: {args.method}")
    print(f"ğŸ“Š Zakres: fragment {start_chunk + 1} - {len(chunks)}")
    
    try:
        for i in range(start_chunk, len(chunks)):
            chunk = chunks[i]
            print(f"ğŸ“ Przetwarzam fragment {i+1}/{len(chunks)}...")
            
            # Generuj Q&A z retry logic
            model_param = args.model_path if args.method == "finetuned" else args.model
            qa_pairs = generate_qa_with_retry(
                chunk, 
                args.method, 
                api_key, 
                model_param
            )
            
            if qa_pairs:
                all_qa_pairs.extend(qa_pairs)
                print(f"   âœ… Wygenerowano {len(qa_pairs)} par Q&A")
            else:
                print(f"   âŒ Brak wynikÃ³w dla fragmentu {i+1}")
            
            # Zapisz postÄ™p
            if i % args.save_every == 0 or qa_pairs:
                save_progress(all_qa_pairs, args.output, i)
            
            # Small delay to respect rate limits
            if args.method in ["openai", "claude"]:
                time.sleep(2)  # Increased for safety
                
    except KeyboardInterrupt:
        print(f"\nâš ï¸  Przerwano przez uÅ¼ytkownika")
        print(f"ğŸ’¾ ZapisujÄ™ ostatni postÄ™p...")
        save_progress(all_qa_pairs, args.output, i)
        print(f"ğŸ“Š Zebrano {len(all_qa_pairs)} par Q&A przed przerwaniem")
        return
        
    except Exception as e:
        print(f"\nâŒ Nieoczekiwany bÅ‚Ä…d: {e}")
        print(f"ğŸ’¾ ZapisujÄ™ ostatni postÄ™p...")
        save_progress(all_qa_pairs, args.output, i)
        return
    
    # Final save and summary
    print(f"\nğŸ“Š PODSUMOWANIE:")
    processed_chunks = len(chunks) - start_chunk if 'start_chunk' in locals() else len(chunks)
    print(f"   Przetworzono fragmentÃ³w: {processed_chunks}/{len(chunks)}")
    print(f"   Wygenerowano par Q&A: {len(all_qa_pairs)}")
    
    if all_qa_pairs:
        # Convert to instruction format
        instruction_data = create_instruction_format(all_qa_pairs)
        
        # Save final results
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(instruction_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Zapisano finalny dataset do: {args.output}")
        print(f"ğŸ“ˆ Format instrukcyjny: {len(instruction_data)} przykÅ‚adÃ³w")
        
        # Show sample
        if instruction_data:
            print(f"\nğŸ” PRZYKÅAD:")
            sample = instruction_data[0]
            print(f"Instrukcja: {sample['instruction']}")
            print(f"OdpowiedÅº: {sample['output'][:200]}...")
        
        # Clean up progress files
        progress_file = args.output.replace('.json', '_progress.json')
        if Path(progress_file).exists():
            print(f"ğŸ§¹ Usuwam pliki postÄ™pu...")
            Path(progress_file).unlink()
            
        print(f"\nğŸ‰ ZAKOÅƒCZONO POMYÅšLNIE!")
        print(f"ğŸ“ Pliki do usuniÄ™cia: *_backup_*.json (opcjonalnie)")
        
    else:
        print(f"âŒ Brak danych Q&A do zapisania")
        
    # Final progress save if needed
    if all_qa_pairs and 'i' in locals():
        save_progress(all_qa_pairs, args.output, i)

if __name__ == "__main__":
    main() 