# ğŸš€ Fine-tuning Bielik-11B z Korpusem Cybernetyki

Przewodnik krok po kroku trenowania polskiego modelu AI na korpusie cybernetyki (Kossecki + Mazur).

## ğŸ“‹ Wymagania

### Hardware:
- **RTX 3090** (24GB VRAM) âœ…
- **32GB+ RAM** (zalecane)
- **50GB+ miejsca** na dysku

### Software:
- **Python 3.10+**
- **CUDA 12.1+**
- **PyTorch 2.1+**

## âš¡ SZYBKI START

### 1. Instalacja zaleÅ¼noÅ›ci
```bash
# Aktywuj environment
pyenv activate cybernetyka-corpus

# Zainstaluj pakiety
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install xformers trl peft accelerate bitsandbytes datasets
```

### 2. Uruchom fine-tuning
```bash
# SprawdÅº czy korpus istnieje
ls -la cybernetyka_corpus.txt

# Uruchom trenowanie (2-6 godzin na RTX 3090)
python3 finetune_bielik.py
```

### 3. Testuj model
```bash
# Po zakoÅ„czeniu trenowania
python3 test_finetuned_model.py
```

## ğŸ”§ OPCJE KONFIGURACJI

### Dla wiÄ™kszej jakoÅ›ci (wiÄ™cej czasu):
Edytuj `finetune_bielik.py`:
```python
training_args = {
    "num_train_epochs": 3,         # ZwiÄ™ksz z 1 do 3
    "learning_rate": 1e-4,         # Zmniejsz learning rate
    "per_device_train_batch_size": 1,  # Zmniejsz batch size
    "r": 32,                       # ZwiÄ™ksz LoRA rank w get_peft_model()
}
```

### Dla szybszego trenowania:
```python
training_args = {
    "num_train_epochs": 1,
    "per_device_train_batch_size": 4,  # ZwiÄ™ksz jeÅ›li masz VRAM
    "max_seq_length": 1024,            # Zmniejsz dÅ‚ugoÅ›Ä‡ sekwencji
}
```

## â±ï¸ CZASY TRENOWANIA na RTX 3090

| Konfiguracja | Epochs | Batch Size | Czas | JakoÅ›Ä‡ |
|--------------|--------|------------|------|--------|
| **Szybka**   | 1      | 4          | 2h   | Dobra  |
| **Standard** | 2      | 2          | 4h   | Bardzo dobra |
| **NajwyÅ¼sza**| 3      | 1          | 8h   | DoskonaÅ‚a |

## ğŸ“Š MONITOROWANIE

### GPU Usage:
```bash
watch nvidia-smi
```

### Logi trenowania:
```bash
tail -f outputs/training.log
```

### TensorBoard (opcjonalne):
```bash
pip install tensorboard
tensorboard --logdir outputs/runs
```

## ğŸ¯ TYPY FINE-TUNINGU

### 1. LoRA (Recommended) â­
- **Szybko:** 2-6h
- **VRAM:** 12-16GB
- **Pliki:** ~1GB
- **JakoÅ›Ä‡:** 95% full fine-tuning

### 2. QLoRA (OszczÄ™dne)
- **Szybko:** 2-8h  
- **VRAM:** 8-12GB
- **Pliki:** ~500MB
- **JakoÅ›Ä‡:** 90% full fine-tuning

### 3. Full Fine-tuning (Nie dla RTX 3090)
- **VRAM:** 40GB+
- **JakoÅ›Ä‡:** 100%

## ğŸ” TROUBLESHOOTING

### CUDA Out of Memory:
```python
# Zmniejsz batch size
"per_device_train_batch_size": 1

# Lub zmniejsz max_seq_length  
"max_seq_length": 1024
```

### Wolne trenowanie:
```python
# ZwiÄ™ksz batch size (jeÅ›li masz VRAM)
"per_device_train_batch_size": 4

# Lub zainstaluj Flash Attention
pip install flash-attn --no-build-isolation
```

### BÅ‚Ä™dy instalacji Unsloth:
```bash
# Alternatywne ÅºrÃ³dÅ‚o
pip install "unsloth @ git+https://github.com/unslothai/unsloth.git"

# Lub uÅ¼yj conda
conda install unsloth -c conda-forge
```

## ğŸ“ STRUKTURA PLIKÃ“W

```
cybernetyka_teksty/
â”œâ”€â”€ cybernetyka_corpus.txt          # Korpus treningowy (11MB)
â”œâ”€â”€ finetune_bielik.py              # Skrypt trenowania  
â”œâ”€â”€ test_finetuned_model.py         # Testy modelu
â”œâ”€â”€ bielik-cybernetyka-lora/        # Wytrenowany model
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors   # LoRA weights (~1GB)
â”‚   â””â”€â”€ tokenizer.json
â”œâ”€â”€ bielik-cybernetyka/             # GGUF dla Ollama
â”‚   â””â”€â”€ model-q4_k_m.gguf          # Kvantyzowany model
â””â”€â”€ outputs/                        # Logi i checkpointy
    â”œâ”€â”€ checkpoint-100/
    â””â”€â”€ training.log
```

## ğŸ§ª TESTOWANIE MODELU

### Automatyczne testy:
```bash
python3 test_finetuned_model.py
```

### RÄ™czne testy:
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained("./bielik-cybernetyka-lora")
FastLanguageModel.for_inference(model)

prompt = "Co to jest cybernetyka spoÅ‚eczna?"
# ... generuj odpowiedÅº
```

## ğŸš€ UÅ»YWANIE W OLLAMA

```bash
# Importuj wytrenowany model
ollama create bielik-cybernetyka -f bielik-cybernetyka/model-q4_k_m.gguf

# Testuj
ollama run bielik-cybernetyka "WyjaÅ›nij teoriÄ™ ukÅ‚adÃ³w samodzielnych Mazura"
```

## ğŸ“ˆ METRYKI SUKCESU

### Dobre trenowanie:
- **Loss:** Spada z ~2.0 do ~0.8
- **Perplexity:** < 3.0 
- **GPU Utilization:** 90%+
- **No OOM errors**

### JakoÅ›Ä‡ odpowiedzi:
- UÅ¼ywa terminologii cybernetycznej
- OdwoÅ‚uje siÄ™ do Kosseckiego/Mazura
- SpÃ³jne z polskÄ… szkoÅ‚Ä… cybernetycznÄ…
- Po polsku, bez bÅ‚Ä™dÃ³w

## ğŸ¯ DALSZE KROKI

1. **ZwiÄ™ksz epochs** dla lepszej jakoÅ›ci
2. **Dodaj instruction tuning** z Q&A
3. **Kvantyzuj** do Q4_K_M dla Ollama
4. **Testuj** na rzeczywistych zadaniach
5. **Iteruj** konfiguracjÄ™

---

**ğŸ’¡ Pro Tip:** Zapisuj checkpointy co 100 krokÃ³w. Fine-tuning moÅ¼na wznowiÄ‡ w przypadku bÅ‚Ä™du!

**ğŸ‰ Po trenowaniu masz specjalistyczny polski model AI w cybernetyce!** 