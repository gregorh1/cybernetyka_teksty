# ğŸš€ Fine-tuning Bielik-11B z Korpusem Cybernetyki

Przewodnik krok po kroku trenowania polskiego modelu AI na korpusie cybernetyki (Kossecki + Mazur).

## ğŸ“‹ Wymagania

### Hardware:
- **RTX 3090** (24GB VRAM) âœ…
- **32GB+ RAM** (zalecane)
- **50GB+ miejsca** na dysku

### Software:
- **Python 3.10+**
- **CUDA 12.1+**
- **PyTorch 2.4+**

## ğŸ”§ METODY FINE-TUNINGU

DostÄ™pne sÄ… **dwie metody** fine-tuningu:

### ğŸ¦¥ **UNSLOTH** (Zalecane jeÅ›li dziaÅ‚a)
- âœ… **2x szybsze** trenowanie (~2-4h)
- âœ… **Mniejsze zuÅ¼ycie VRAM** (moÅ¼e 8-bit)
- âœ… **Wbudowane optymalizacje** (Flash Attention)
- âŒ **Problemy z kompatybilnoÅ›ciÄ…** PyTorch/CUDA
- âŒ **Niestabilne** z nowymi wersjami

### ğŸ”§ **STANDARD** (Zawsze dziaÅ‚a)
- âœ… **Wysoka kompatybilnoÅ›Ä‡** wszystkich wersji
- âœ… **Stabilne i niezawodne**
- âœ… **Åatwiejsze debugowanie**
- âŒ **Wolniejsze trenowanie** (~3-6h)
- âŒ **Wymaga 4-bit quantization**

## ğŸš€ **AUTOMATYCZNY START**

```bash
# Uruchom skrypt - wybierze najlepszÄ… metodÄ™ automatycznie
./run_finetune.sh
```

Skrypt:
1. âœ… Sprawdzi Å›rodowisko
2. ğŸ§ª Przetestuje dostÄ™pnoÅ›Ä‡ Unsloth
3. ğŸ¤– Pozwoli wybraÄ‡ metodÄ™ (jeÅ›li obie dziaÅ‚ajÄ…)
4. ğŸ”¥ Uruchomi trenowanie
5. ğŸ§ª Przetestuje wytrenowany model

## âš¡ SZYBKI START - METODA UNSLOTH

### 1. Instalacja zaleÅ¼noÅ›ci
```bash
# Aktywuj environment
pyenv activate cybernetyka-corpus

# Zainstaluj pakiety dla Unsloth
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install transformers datasets accelerate peft trl bitsandbytes
pip install flash-attn --no-build-isolation
```

### 2. Uruchom fine-tuning
```bash
python3 finetune_bielik.py
```

## ğŸ”§ SZYBKI START - METODA STANDARD

### 1. Instalacja zaleÅ¼noÅ›ci
```bash
# Aktywuj environment
pyenv activate cybernetyka-corpus

# Zainstaluj pakiety dla Standard
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets accelerate peft trl bitsandbytes
pip install "numpy<2"  # KompatybilnoÅ›Ä‡
```

### 2. Uruchom fine-tuning
```bash
python3 finetune_bielik_standard.py
```

## ğŸ“Š PORÃ“WNANIE METOD

| Aspekt | ğŸ¦¥ Unsloth | ğŸ”§ Standard |
|--------|-----------|------------|
| **Czas trenowania** | 2-4h | 3-6h |
| **VRAM na RTX 3090** | 16-20GB | 22-24GB |
| **KompatybilnoÅ›Ä‡** | Problematyczna | Wysoka |
| **StabilnoÅ›Ä‡** | Niestabilna | Bardzo stabilna |
| **JakoÅ›Ä‡ modelu** | Identyczna | Identyczna |
| **Quantization** | 8-bit moÅ¼liwa | 4-bit wymagana |
| **Debugowanie** | Trudne | Åatwe |
| **Dla produkcji** | Ryzykowne | Zalecane |

## ğŸ“ PLIKI SKRYPTÃ“W

```
cybernetyka_teksty/
â”œâ”€â”€ run_finetune.sh                 # ğŸš€ GÅ‚Ã³wny skrypt (AUTO-WYBÃ“R)
â”œâ”€â”€ finetune_bielik.py              # ğŸ¦¥ Unsloth (szybkie)
â”œâ”€â”€ finetune_bielik_standard.py     # ğŸ”§ Standard (stabilne)
â”œâ”€â”€ test_finetuned_model.py         # ğŸ§ª Testy modelu
â””â”€â”€ setup_environment.py            # ğŸ“¦ Setup pakietÃ³w
```

## â±ï¸ CZASY TRENOWANIA na RTX 3090

### ğŸ¦¥ Unsloth:
| Konfiguracja | Epochs | Batch Size | VRAM | Czas |
|--------------|--------|------------|------|------|
| **Szybka**   | 1      | 4          | 16GB | 1.5h |
| **Standard** | 2      | 2          | 18GB | 3h   |
| **NajwyÅ¼sza**| 3      | 1          | 20GB | 4.5h |

### ğŸ”§ Standard:
| Konfiguracja | Epochs | Batch Size | VRAM | Czas |
|--------------|--------|------------|------|------|
| **Szybka**   | 1      | 2          | 22GB | 3h   |
| **Standard** | 2      | 1          | 23GB | 5h   |
| **NajwyÅ¼sza**| 3      | 1          | 24GB | 7.5h |

## ğŸ”§ OPCJE KONFIGURACJI

### Dla wiÄ™kszej jakoÅ›ci (wiÄ™cej czasu):
```python
# W obu skryptach
training_args = {
    "num_train_epochs": 3,         # ZwiÄ™ksz z 1 do 3
    "learning_rate": 1e-4,         # Zmniejsz learning rate
    "per_device_train_batch_size": 1,  # Zmniejsz batch size
}

# W LoRA config
lora_config = {
    "r": 32,                       # ZwiÄ™ksz LoRA rank
    "lora_alpha": 64,              # ZwiÄ™ksz alpha
}
```

### Dla szybszego trenowania:
```python
training_args = {
    "num_train_epochs": 1,
    "per_device_train_batch_size": 2,  # ZwiÄ™ksz jeÅ›li masz VRAM
    "max_seq_length": 1024,            # Zmniejsz dÅ‚ugoÅ›Ä‡
}
```

## ğŸ“Š MONITOROWANIE

### GPU Usage:
```bash
watch nvidia-smi
```

### Logi trenowania:
```bash
tail -f outputs/training.log
```

### TensorBoard (opcjonalne):
```bash
pip install tensorboard
tensorboard --logdir outputs/runs
```

## ğŸ” TROUBLESHOOTING

### âŒ Unsloth nie dziaÅ‚a:
```bash
# UÅ¼yj Standard
python3 finetune_bielik_standard.py

# Lub napraw wersje PyTorch
pip uninstall torch torchvision torchaudio
pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121
```

### âŒ CUDA Out of Memory:
```python
# Zmniejsz batch size
"per_device_train_batch_size": 1

# Zmniejsz max_seq_length  
"max_seq_length": 1024

# UÅ¼yj gradient checkpointing
"gradient_checkpointing": True
```

### âŒ Flash Attention bÅ‚Ä™dy:
```bash
# Reinstall dla odpowiedniej wersji PyTorch
pip uninstall flash-attn
pip install flash-attn --no-build-isolation
```

### âŒ NumPy incompatibility:
```bash
pip install "numpy<2"
```

## ğŸ“ STRUKTURA PLIKÃ“W PO TRENOWANIU

```
cybernetyka_teksty/
â”œâ”€â”€ cybernetyka_corpus.txt          # Korpus treningowy (11MB)
â”œâ”€â”€ bielik-cybernetyka-lora/        # Wytrenowany model
â”‚   â”œâ”€â”€ adapter_config.json         # Konfiguracja LoRA
â”‚   â”œâ”€â”€ adapter_model.safetensors   # LoRA weights (~1GB)
â”‚   â”œâ”€â”€ tokenizer.json              # Tokenizer
â”‚   â””â”€â”€ training_args.bin           # Argumenty trenowania
â”œâ”€â”€ outputs/                        # Logi i checkpointy
â”‚   â”œâ”€â”€ checkpoint-100/             # Checkpointy
â”‚   â”œâ”€â”€ training.log                # Logi trenowania
â”‚   â””â”€â”€ runs/                       # TensorBoard logs
â””â”€â”€ bielik-cybernetyka/             # GGUF dla Ollama (opcjonalne)
    â””â”€â”€ model-q4_k_m.gguf          # Kvantyzowany model
```

## ğŸ§ª TESTOWANIE MODELU

### Automatyczne testy:
```bash
python3 test_finetuned_model.py
```

### RÄ™czne testy - Unsloth:
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained("./bielik-cybernetyka-lora")
FastLanguageModel.for_inference(model)

prompt = "Co to jest cybernetyka spoÅ‚eczna?"
# ... generuj odpowiedÅº
```

### RÄ™czne testy - Standard:
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("speakleash/Bielik-11B-v2.2-Instruct")
model = PeftModel.from_pretrained(base_model, "./bielik-cybernetyka-lora")
tokenizer = AutoTokenizer.from_pretrained("./bielik-cybernetyka-lora")

prompt = "Co to jest cybernetyka spoÅ‚eczna?"
# ... generuj odpowiedÅº
```

## ğŸš€ UÅ»YWANIE W OLLAMA

```bash
# Importuj wytrenowany model (wymaga konwersji do GGUF)
ollama create bielik-cybernetyka -f bielik-cybernetyka/model-q4_k_m.gguf

# Testuj
ollama run bielik-cybernetyka "WyjaÅ›nij teoriÄ™ ukÅ‚adÃ³w samodzielnych Mazura"
```

## ğŸ“ˆ METRYKI SUKCESU

### Dobre trenowanie:
- **Loss:** Spada z ~2.0 do ~0.8
- **Perplexity:** < 3.0 
- **GPU Utilization:** 90%+
- **No OOM errors**

### JakoÅ›Ä‡ odpowiedzi:
- UÅ¼ywa terminologii cybernetycznej
- OdwoÅ‚uje siÄ™ do Kosseckiego/Mazura
- SpÃ³jne z polskÄ… szkoÅ‚Ä… cybernetycznÄ…
- Po polsku, bez bÅ‚Ä™dÃ³w

## ğŸ¯ KTÃ“RA METODA WYBRAÄ†?

### Wybierz **ğŸ¦¥ Unsloth** jeÅ›li:
- âœ… Chcesz szybkie trenowanie
- âœ… Masz duÅ¼o czasu na debugowanie
- âœ… Experimentujesz z parametrami
- âœ… Nie jest to Å›rodowisko produkcyjne

### Wybierz **ğŸ”§ Standard** jeÅ›li:
- âœ… Potrzebujesz stabilnoÅ›ci
- âœ… To Å›rodowisko produkcyjne
- âœ… Masz ograniczony czas na troubleshooting
- âœ… Unsloth nie dziaÅ‚a na twoim systemie

## ğŸ¯ DALSZE KROKI

1. **ZwiÄ™ksz epochs** dla lepszej jakoÅ›ci
2. **Dodaj instruction tuning** z Q&A
3. **Kvantyzuj** do Q4_K_M dla Ollama
4. **Testuj** na rzeczywistych zadaniach
5. **Iteruj** konfiguracjÄ™

---

**ğŸ’¡ Pro Tip:** Skrypt `run_finetune.sh` automatycznie wybierze najlepszÄ… metodÄ™ dla twojego systemu!

**ğŸ‰ Po trenowaniu masz specjalistyczny polski model AI w cybernetyce!** 